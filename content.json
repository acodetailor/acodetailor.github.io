{"meta":{"title":"Sam","subtitle":"用我的双手，成就你的梦想。一库~","description":"杂记","author":"Sam","url":"https://acodetailor.github.io","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2021-03-30T07:24:39.640Z","updated":"2021-03-30T07:24:39.640Z","comments":false,"path":"/404.html","permalink":"https://acodetailor.github.io/404.html","excerpt":"","text":""},{"title":"书单","date":"2021-03-30T07:24:39.645Z","updated":"2021-03-30T07:24:39.644Z","comments":false,"path":"books/index.html","permalink":"https://acodetailor.github.io/books/index.html","excerpt":"","text":""},{"title":"关于","date":"2021-03-30T07:24:39.644Z","updated":"2021-03-30T07:24:39.643Z","comments":false,"path":"about/index.html","permalink":"https://acodetailor.github.io/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"分类","date":"2021-03-30T07:24:39.645Z","updated":"2021-03-30T07:24:39.645Z","comments":false,"path":"categories/index.html","permalink":"https://acodetailor.github.io/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2021-03-30T07:24:39.646Z","updated":"2021-03-30T07:24:39.646Z","comments":true,"path":"links/index.html","permalink":"https://acodetailor.github.io/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2021-03-30T07:24:39.647Z","updated":"2021-03-30T07:24:39.647Z","comments":false,"path":"repository/index.html","permalink":"https://acodetailor.github.io/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2021-03-30T07:24:39.648Z","updated":"2021-03-30T07:24:39.648Z","comments":false,"path":"tags/index.html","permalink":"https://acodetailor.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"consul nodeId conflict导致所有agent都无法加入","slug":"consul_prolem","date":"2022-06-01T08:39:43.013Z","updated":"2022-06-01T08:58:17.824Z","comments":false,"path":"2022/06/01/consul_prolem/","link":"","permalink":"https://acodetailor.github.io/2022/06/01/consul_prolem/","excerpt":"","text":"一、现象周一刚上班，开发环境出问题了，大家反馈的本地的consul client无法加入到server集群了。由于我最近搞过这方面，让帮忙看看。报错日志（网上找的）： 1Failed to join xx.xxx.xxx.xxx: Member &#x27;A&#x27; has conflicting node ID &#x27;fd002068-1f26-7e90-ba80-b9e999d874a1&#x27; with B. 这一看很明显啊，有2位同事的nodeID冲突了，改一下就解决了。结果一个同事请假了，另一个更神奇了，好像不是我们项目组的。由于项目组内注册时都使用拼音缩写（后来想想使用工号貌似更好一点），看了一圈确定不是本项目的。联系相关同事，通过ip反查肇事者，让他改了本地的配置。 二、解决问题原因：1、这位肇事者使用了别人给的配置，没有修改server导致直接注册到我们的consul了。2、由于是开发环境，管控不严，实际应该开启ACL策略，只有分配token了才能注册。3、nodeId正常是根据client端的bios硬件id转化来的，里面是通过读取/sys/class/dmi/id/product_uuid得到一个id，由于我们使用的云服务器，可能导致重复吧4、可以在client加入集群时指定nodeId,避免重复。 三、走进科学事情解决了之后就在考虑，为啥别人的nodeId冲突了会导致无关的人也无法加入consul server集群呢？是feature?但是感觉还是说不通，果然之前有人碰到过，给consul提过issue了。看了一下好像从1.5.x 到1.7.x都有人碰到。上链接：https://github.com/hashicorp/consul/issues/7396#issuecomment-1141855713","categories":[{"name":"consul","slug":"consul","permalink":"https://acodetailor.github.io/categories/consul/"}],"tags":[{"name":"consul","slug":"consul","permalink":"https://acodetailor.github.io/tags/consul/"}]},{"title":"Feign了解一下","slug":"feign","date":"2022-05-28T07:49:20.101Z","updated":"2022-05-29T07:32:19.551Z","comments":false,"path":"2022/05/28/feign/","link":"","permalink":"https://acodetailor.github.io/2022/05/28/feign/","excerpt":"","text":"一、背景最近看到一个项目。使用Feign实现微服务调用，由于注册到服务中心定的服务名字需要规范化，所以微服务涉及Feign调用的需要修改一下Feign调用的微服务名。项目里的微服务里有通过Feign调用微服务的，也有调用别的系统接口的。然后菜鸟就看懵了。下边是例子：================ABC含义以及关系，微服务：A调用B，三方系统：C微服务A的Feign调用代码Demo，分别标识4个Interface文件。 12345@FeignClient(name = &quot;B&quot;,url = &quot;&quot;)public interface feign1 &#123; @RequestMapping(path = &quot;test/demo&quot;,method = RequestMethod.GET) public void getUser(String name);&#125; 12345@FeignClient(value = &quot;C&quot;,url = &quot;http://127.0.0.1:8080&quot;)public interface feign2 &#123; @RequestMapping(path = &quot;test/demo&quot;,method = RequestMethod.GET) public void getUser(String name);&#125; 12345@FeignClient(value = &quot;C&quot;,url = &quot;http://127.0.0.1:8080&quot;)public interface feign3 &#123; @RequestMapping(path = &quot;test1/demo&quot;,method = RequestMethod.GET) public void getUser(String name);&#125; 123456@FeignClient(name = &quot;A&quot;,url = &quot;http://127.0.0.1:8080&quot;)public interface feign4 &#123; @Get @Path(&quot;test2/demo&quot;) public void getUser(String name);&#125; 看了以上代码后菜鸟疑问：1、feign name可以定义成一样的？为啥不合成一个文件？？2、为啥调用C的时候，name要写A呢，保持一致多好？？？3、为啥要混用JAX-RS注解和SpringMVC的注解呢？？？？ 算了，先改名称吧，我就改个名称而已，由于菜鸟的偶发性强迫症（没有自知之明），顺便改了一下2，改成了下边这样 12345@FeignClient(name = &quot;B&quot;,url = &quot;&quot;)public interface feign1 &#123; @RequestMapping(path = &quot;test/demo&quot;,method = RequestMethod.GET) public void getUser(String name);&#125; 12345@FeignClient(value = &quot;C&quot;,url = &quot;http://127.0.0.1:8080&quot;)public interface feign2 &#123; @RequestMapping(path = &quot;test/demo&quot;,method = RequestMethod.GET) public void getUser(String name);&#125; 12345@FeignClient(value = &quot;C&quot;,url = &quot;http://127.0.0.1:8080&quot;)public interface feign3 &#123; @RequestMapping(path = &quot;test1/demo&quot;,method = RequestMethod.GET) public void getUser(String name);&#125; 123456@FeignClient(name = &quot;C&quot;,url = &quot;http://127.0.0.1:8080&quot;)public interface feign4 &#123; @Get @Path(&quot;test2/demo&quot;) public void getUser(String name);&#125; 然后打包运行，结果启动失败了。。。就改个名字就导致启动失败了？？和我改的没关系吧，要不是只有我用这个分支，我都怀疑是有人要害我。接受是我改名字导致的问题后，这就得较较劲了，开启快乐网上冲浪时刻。 二、网上冲浪网上搜到了以下Feign配置方式1、name、value、serviceId配置作用一样，一般是当做Feign注册的Bean的Name。如果Bean Name冲突了，可以使用spring.main.allow-bean-definition-overriding=true配置、contextId、合成一个bean来解决。2、配置了name，url为空时,通过name获取注册中心服务的实例进行访问。3、配置了name，url不为空时，直接通过url进行访问。4、最好不要混合使用注解，如一个服务里同时用JAX-RS和SpringMVC的注解。 三、走进科学1、name、value 1234567891011121314151617181920private String getClientName(Map&lt;String, Object&gt; client) &#123; if (client == null) &#123; return null; &#125; else &#123; String value = (String)client.get(&quot;value&quot;); if (!StringUtils.hasText(value)) &#123; value = (String)client.get(&quot;name&quot;); &#125; if (!StringUtils.hasText(value)) &#123; value = (String)client.get(&quot;serviceId&quot;); &#125; if (StringUtils.hasText(value)) &#123; return value; &#125; else &#123; throw new IllegalStateException(&quot;Either &#x27;name&#x27; or &#x27;value&#x27; must be provided in @&quot; + FeignClient.class.getSimpleName()); &#125; &#125; &#125; 2、allow-bean-definition-overriding=true的作用。允许Spring覆盖同名的Bean，一般不建议使用这种方式解决Feign重复Bean问题，使用contextId或者合到一个Bean里3、url注册 1234567891011121314151617181920212223242526272829303132public Object getObject() throws Exception &#123; FeignContext context = (FeignContext)this.applicationContext.getBean(FeignContext.class); Builder builder = this.feign(context); String url; if (!StringUtils.hasText(this.url)) &#123; if (!this.name.startsWith(&quot;http&quot;)) &#123; url = &quot;http://&quot; + this.name; &#125; else &#123; url = this.name; &#125; url = url + this.cleanPath(); return this.loadBalance(builder, context, new HardCodedTarget(this.type, this.name, url)); &#125; else &#123; if (StringUtils.hasText(this.url) &amp;&amp; !this.url.startsWith(&quot;http&quot;)) &#123; this.url = &quot;http://&quot; + this.url; &#125; url = this.url + this.cleanPath(); Client client = (Client)this.getOptional(context, Client.class); if (client != null) &#123; if (client instanceof LoadBalancerFeignClient) &#123; client = ((LoadBalancerFeignClient)client).getDelegate(); &#125; builder.client(client); &#125; Targeter targeter = (Targeter)this.get(context, Targeter.class); return targeter.target(this, builder, context, new HardCodedTarget(this.type, this.name, url)); &#125; &#125; 4、关于上边的疑问呢，也无从考证，祖传代码，存在即合理。正常一个对一个源的Feign调用可以声明在一个Bean里，对于混合使用注解，感觉也没有必要。除非是源的协议、Decode、Encode等方式不一样，可以针对不同的Bean配置不同的契约。","categories":[{"name":"Java","slug":"Java","permalink":"https://acodetailor.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://acodetailor.github.io/tags/Java/"}]},{"title":"Mysql常用数据结构","slug":"数据结构","date":"2022-05-22T12:44:09.575Z","updated":"2022-05-22T13:26:12.570Z","comments":false,"path":"2022/05/22/数据结构/","link":"","permalink":"https://acodetailor.github.io/2022/05/22/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","excerpt":"","text":"一、Mysql常用的数据结构 Q1: int(10)和int(11)有啥区别? A1: 不会影响存入时的数据范围，显示宽度只是指明 mysql 最大可能显示的数字个数，数值的位数小于指定的宽度时就由空格填充， 如果插入了大于显示宽度的值，只要该值不超过该类型整数的取值范围，数值依然可以插入，而且能够显示出来。 备注：1、varchar是可变长度，使用多大空间和声明的长度没关系，和实际传入的数据长度有关。2、现在使用的5.X版本，varchar长度指定的是字符，超过会自动转为text 二、复习基础知识位（比特位）：bit（binary digit）（简写：b），是计算机数据存储最小的单位，二进制中，0或者1就是一个位（比特位）bit。 字节：Byte（简写：B），是计算机信息技术用于计量存储容量的一种计量单位，通常情况下一字节等于八位，也就是 → 1Byte = 8bit 字符：Character，在计算机和电信技术中，一个字符是一个单位的字形、类字形单位或符号的基本信息。 字符与字节： ASCII码：一个英文字母（不区分大小写）占一个字节的空间。如一个ASCII码就是一个字节。 UTF-8编码：一个英文字符等于一个字节，一个中文等于三个字节。中文标点占三个字节，英文标点占一个字节。 Unicode编码：一个英文等于两个字节，一个中文等于两个字节。中文标点占两个字节，英文标点占两个字节。 gbk 编码下：一个英文占用一个字节，一个汉字占用 2 个 字节。","categories":[{"name":"DB","slug":"DB","permalink":"https://acodetailor.github.io/categories/DB/"}],"tags":[{"name":"cache","slug":"cache","permalink":"https://acodetailor.github.io/tags/cache/"}]},{"title":"缓存一致性策略","slug":"cache_consistent","date":"2022-05-22T07:32:35.712Z","updated":"2022-05-22T08:46:58.604Z","comments":false,"path":"2022/05/22/cache_consistent/","link":"","permalink":"https://acodetailor.github.io/2022/05/22/cache_consistent/","excerpt":"","text":"一、问题在当前互联网系统中，为了提高响应速度，使用缓存是大家最常用的手段。但是使用缓存后必然带来了缓存和数据库的一致性问题，下边从读和改两个方面，就几种选择分析一下存在的问题。 二、Cache-Aside策略2.1 读读的话比较简单，一般都是先读取缓存，如果缓存没有命中，则查询数据库，然后将数据添加到缓存，返回数据 2.2 改方案①先改缓存，再改数据库并发修改数据时，A先修改了缓存，B修改了缓存、修改了数据库、A修改数据库，造成数据不一致。 方案②先改数据库，再改缓存并发时也存在上述问题。 方案③先删缓存，再改数据库并发时A读，B改，B删除-&gt; A 读取（更新了缓存）-&gt; B改数据库，造成不一致。读操作正常会比改快一些，高并发下还是容易出现问题。 方案④先改数据库，再删缓存并发时，A读，B改。 A读 -&gt; B改 -&gt; B删 -&gt; A(更新)。造成不一致。 极端条件下确实存在该问题，但是读正常会比改从操作快，满足改问题出现的条件还是比较苛刻，所以一般选择该方案。假设删除缓存失败了呢，暂时不考虑事务场景。可以使用消息队列进行补偿，删除失败后将key添加到消息队列里。 三、Read Through和Write Through这种方案将缓存作为主要数据存储，缓存数据不存在过期时间。 Read-Through，应用查询缓存是否存在，存在则返回；不存在则由缓存组件去数据库加载数据。Write-Through，先查询要写入的数据在缓存是否存在，存在则先更新缓存然后再更新数据库最后返回。 适合读多写少的场景，比较缓存中存了所有数据。 四、Write BackWrite Back和上述的方案区别在于缓存和数据库的同步通过异步方案来处理。","categories":[{"name":"DB","slug":"DB","permalink":"https://acodetailor.github.io/categories/DB/"}],"tags":[{"name":"cache","slug":"cache","permalink":"https://acodetailor.github.io/tags/cache/"}]},{"title":"你真的了解TCP吗","slug":"tcp","date":"2022-05-10T09:30:53.521Z","updated":"2022-05-15T13:02:51.416Z","comments":false,"path":"2022/05/10/tcp/","link":"","permalink":"https://acodetailor.github.io/2022/05/10/tcp/","excerpt":"","text":"0、写在之前准备继续写博了，起因是看到一位年轻且有实力的博主的一段话，大体意思如下：写博客不希望多少人能看到，但是几十年后自己能看看走过的路也挺好，然后感觉被什么击中了。个人年前工作进行了调整，技术栈有了很大的变化，正好也可以记录一下对新技术栈的学习使用过程。 一、为什么TCP建连需要三次连接TCP作为常见的面试题，大多数会问题三次握手、四次挥手、time_wait、close_wait状态在什么情况下会出现。有考虑过为啥要三次握手？ ① TCP连接是什么？借用一下别人的总结：TCP连接是用于保证可靠性和流控制机制的信息，包括 Socket、序列号以及窗口大小。 ② 假设2次握手如果是2次握手，client端发起请求，但是server端只能选择接收或者拒绝，当在网络波动时，client端重复发送请求时，server端如何判断是历史请求还是当前请求呢？当使用三次握手来建立连接并在连接引入了 RST 这一控制消息，接收方当收到请求时会将发送方发来的 SEQ+1 发送给对方，这时由发送方来判断当前连接是否是历史连接：如果当前连接是历史连接，即 SEQ 过期或者超时，那么发送方就会直接发送 RST 控制消息中止这一次连接；如果当前连接不是历史连接，那么发送方就会发送 ACK 控制消息，通信双方就会成功建立连接； ③ 假设四次握手四次握手，比如cient发送syn请求，server先回复ack确认包，再发送syn包，最后client再发送ack包，这样四次握手也可以正常建连。其实设计者在追求的建立连接的最少次数，因为总可以使用更多的次数来交换相同的信息。 二、握手时的异常情况① client的第一个syn包丢了如果客户端第一个SYN包丢了，TCP 协议中，某端的在一定时间范围内，只要没有收到应答的包，无论是请求包对方没有收到，还是对方的应答包自己没有收到，均认定为是丢包了，会触发超时重传机制。所以此时会重传SYN包。根据《TCP/IP详解卷Ⅰ：协议》中的描述，会尝试三次，间隔时间分别是 5.8s、24s、48s，总的尝试时间是 75s。 ② server回复的syn+ack包丢了此时服务端已经收到了数据包并回复，如果这个包丢了，如第一步说的，client会认为自己的syn包丢了，client就会继续重传。server端在一定时间内没有收到客户端发来的ack包，也会触发重传，此时server处于 SYN_RCVD 状态，会依次等待 3s、6s、12s 后，重新发送SYN,ACK包。而这个重传次数，linux下可以通过 tcp_synack_retries 进行配置，默认值为 5。如果这个重试次数内，仍未收到，那么服务端会自动关闭这个连接。同时由于client也会进行重传，server收到后，会立即重新发送SYN+ACK包。 ③ client的syn+ack包丢了如果最后一个包丢了，server因为收不到走重传机制，而client ESTABLISHED 状态。client进入 ESTABLISHED 状态后，认为连接已成功建立，会立即发送数据。但是服务端因为没有收到最后一个ACK包，依然处于 SYN-RCVD 状态。那么server收到了client的包会怎么处理呢？其实当客户端在 ESTABLISHED 状态下，开始发送数据包时，会携带上一个ACK的确认序号，所以三次握手的最后一个包丢了，服务端在收到这个数据包时，能够通过包内 ACK 的确认序号，正常进入 ESTABLISHED 状态 三、3次握手时server端有syn_rcvd和ESTABLISHED状态，真实场景下会有很多连接同时建立，server端是怎么处理的呢半连接队列（Incomplete connection queue），又称 SYN 队列全连接队列（Completed connection queue），又称 Accept 队列 当客户端发起 SYN 到服务端，服务端收到以后会回 ACK 和自己的 SYN。这时服务端这边的 TCP 从 listen 状态变为 SYN_RCVD (SYN Received)，此时会将这个连接信息放入半连接队列，半连接队列也被称为 SYN Queue。服务端回复 SYN+ACK 包以后等待客户端回复 ACK，同时开启一个定时器，如果超时还未收到 ACK 会进行 SYN+ACK 的重传，重传的次数由 tcp_synack_retries值确定（和上边说的对上了）。一旦收到客户端的 ACK，服务端就开始尝试把它加入另外一个全连接队列（Accept Queue）。半连接队列的大小与三个值有关：用户层 listen 传入的backlog系统变量 net.ipv4.tcp_max_syn_backlog，默认值为 128系统变量 net.core.somaxconn，默认值为 128 全连接队列包含了服务端所有完成了三次握手，但是还未被应用调用 accept 取走的连接队列。此时的 socket 处于 ESTABLISHED 状态。每次应用调用 accept() 函数会移除队列头的连接。如果队列为空，accept() 通常会阻塞。全连接队列也被称为 Accept 队列。TCP 全连接队列的最大长度由 min(somaxconn, backlog) 控制，其中：somaxconn 是 Linux 内核参数，由 /proc/sys/net/core/somaxconn 指定backlog 是 TCP 协议中 listen 函数的参数之一，即 int listen(int sockfd, int backlog) 函数中的 backlog 大小。在 Golang 中，listen 的 backlog 参数使用的是 /proc/sys/net/core/somaxconn 文件中的值。 backlog既然决定了队列的长度，那在我们优化并发的处理上可以通过他进行调优，Tomcat、nginx、redis等都用到了，感兴趣可以搜搜。遇到半连接、全连接队列溢出查看命令和回显 123$ netstat -s | grep -i &quot;listen&quot; 189088 times the listen queue of a socket overflowed 30140232 SYNs to LISTEN sockets dropped 四、配个图 (网上复制的)","categories":[{"name":"network","slug":"network","permalink":"https://acodetailor.github.io/categories/network/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://acodetailor.github.io/tags/Linux/"}]},{"title":"Knative","slug":"knative源码一","date":"2021-09-22T03:25:42.493Z","updated":"2021-09-25T15:05:42.758Z","comments":false,"path":"2021/09/22/knative源码一/","link":"","permalink":"https://acodetailor.github.io/2021/09/22/knative%E6%BA%90%E7%A0%81%E4%B8%80/","excerpt":"","text":"一、Knative是什么Knative分为三个部分，Serving、Eventing、Building。Serving：主要使用Knative Service部署应用，具有弹性伸缩等功能。Eventing：事件驱动，通过其他内置的资源，实现事件的异步流转。Building：镜像构建。 二、Serving架构 如上图所示，当我们手动创建一个Knative的Service资源时，Knative的controller会自动给我们创建如图所示的资源，实现Serving组件的相关功能。 后续详细分析各个controller代码。","categories":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/categories/k8s/"}],"tags":[{"name":"knative","slug":"knative","permalink":"https://acodetailor.github.io/tags/knative/"}]},{"title":"Camelk","slug":"camel","date":"2021-09-13T15:46:34.407Z","updated":"2021-09-22T03:25:42.493Z","comments":false,"path":"2021/09/13/camel/","link":"","permalink":"https://acodetailor.github.io/2021/09/13/camel/","excerpt":"","text":"一、简介Camel这个词想必很多java的开发者都很熟悉了，它是一个开源的集成框架，可以通过它快速的使用第三方的组件、调用其他系统的接口。这篇文章要介绍的camelk，其实是Camel+Knative的组合。通过CRD将Camel的组件封装成k8s集群的资源，CRD Controller创建Knative的Service资源，可以同时使用到Knative带来的弹性伸缩、事件驱动等技术优点。 二、使用下面使用一个例子来进行简单介绍。需求:通过rest接口提供数据库表的查询。实现:延伸:实现还是比较简单的，也不需要重新构建发布。延伸看来，我们可以通过这种方式快速实现数据集成、服务暴露等需求。 三、架构","categories":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/categories/k8s/"}],"tags":[{"name":"oom","slug":"oom","permalink":"https://acodetailor.github.io/tags/oom/"}]},{"title":"Serverless之Knative","slug":"serverless-istio","date":"2021-05-16T12:59:42.740Z","updated":"2021-05-16T13:48:47.655Z","comments":false,"path":"2021/05/16/serverless-istio/","link":"","permalink":"https://acodetailor.github.io/2021/05/16/serverless-istio/","excerpt":"","text":"一、什么是Knativeknative 是谷歌牵头的 serverless 架构方案，旨在提供一套简单易用的 serverless 开源方案，把 serverless 标准化和平台化。目前参与 knative 项目的公司主要有： Google、Pivotal、IBM、Red Hat和SAP。 这是 Google Cloud Platform 宣布 knative 时给出的介绍：Developed in close partnership with Pivotal, IBM, Red Hat, and SAP, Knative pushes Kubernetes-based computing forward by providing the building blocks you need to build and deploy modern, container-based serverless applications.与Pivotal，IBM，Red Hat和SAP密切合作开发，通过提供构建和部署基于容器的现代serverless应用程序所需的构建块，Knative推动基于Kubernetes的计算。 在knative的github页面，Knative 给出的官方介绍如下：Kubernetes-based platform to build, deploy, and manage modern serverless workloads.基于Kubernetes的平台，用于构建，部署和管理现代serverless工作负载。 二、Knative概述Knative扩展了Kubernetes，提供了一组中间件组件，这些组件对于构建可在任何地方运行的现代以源为中心和基于容器的应用程序至关重要：在本地，在云中，甚至在第三方数据中心。 Knative项目下的每个组件都尝试识别常见模式并编纂最佳实践，这些最佳实践被真实世界中基于Kubernetes的成功框架和应用程序共享。 Knative组件专注于解决许多平凡但困难的任务，例如： 部署容器 在Kubernetes上编排source-to-URL的工作流程 使用蓝绿部署路由和管理流量 根据需求自动收缩和调整工作负载大小 将运行服务绑定到事件生态系统 Knative的开发人员可以使用熟悉的习语，语言和框架来部署任何工作负载：函数，应用程序或容器。 组件 Build - 源到容器的构建编排 Eventing - 管理和交付事件 Serving - 请求驱动的计算，可以扩展到零 受众 开发人员Knative组件为开发人员提供Kubernetes原生API，用于将serverless风格的函数，应用程序和容器部署到自动伸缩运行时。 运维Knative组件旨在集成到更加优雅的产品中，云服务提供商或大型企业的内部团队可以随后运维。 任何企业或云提供商都可以将Knative组件应用到他们自己的系统中，并将这些收益传递给他们的客户。 Knative是一个多元化，开放和包容的社区。 三、Knative优势和已有的FaaS/serverless实现不同，knative在产品规划和设计理念上有带来新东西： 工作负载类型和标准化的 FaaS 不同，knative 期望能够运行所有的 workload : traditional application function container 针对常见应用用例提供更高级别抽象的聚焦API。 在几秒钟内即可提供可扩展，安全，无状态的服务。 松耦合特性可以按需使用组件 可插拔组件，可以自备日志和监控，网络和服务网格。Knative是可移植的：可运行于Kubernetes运行的任何地方，不用担心供应商锁定。knative 建立在 kubernetes 和 istio 之上 使用 kubernetes 提供的容器管理能力（deployment、replicaset、和 pods等），以及 istio 提供的网络管理功能（ingress、LB、dynamic route等）。 需要特别强调的是：knative 是 Kubernetes-based！ 或者说 Kubernetes-only，仅仅运行于k8s平台。","categories":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/tags/k8s/"}]},{"title":"Serverless之Knative流量转发","slug":"knative","date":"2021-04-30T01:40:25.368Z","updated":"2021-05-18T07:26:34.581Z","comments":false,"path":"2021/04/30/knative/","link":"","permalink":"https://acodetailor.github.io/2021/04/30/knative/","excerpt":"","text":"前边说过Knative是基于K8s部署的，使用k8s的容器管理功能以及istio的网络管理功能，下边一起看一下Knative的流量是怎么转发的吧。 一、Knative资源介绍Knative资源knative主要分为三个组件，build、server、event，分别处理CI/CD,服务伸缩，事件驱动。流量转发主要由server这部分处理。 knative service knative ingress knative serverlessService knative route knative configuration kantive revision istio资源 Gateway virtualService。 二、流量转发官方架构先回顾一下Knative官方的一个简单的原理示意图如下所示。用户创建一个Knative Service（ksvc）后，Knative会自动创建Route（route）、Configuration（cfg）资源，然后cfg会创建对应的Revision（rev）版本。rev实际上又会创建Deployment提供服务，流量最终会根据route的配置，导入到相应的rev中。 老版本（0.6）在集成使用Istio部署时，Route默认采用的是Istio Ingress Gateway实现，大概在Knative 0.6版本之前，我们可以发现，Route的流量转发本质上是由Istio virtualservice（vs）控制。副本数为0时，其中destination指向的是Activator组件。此时Activator会帮助转发冷启动时的请求。 12345678910111213141516171819apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: route-f8c50d56-3f47-11e9-9a9a-08002715c9e6spec: gateways: - knative-ingress-gateway - mesh hosts: - helloworld-go.default.example.com - helloworld-go.default.svc.cluster.local http: - appendHeaders: route: - destination: host: Activator-Service.knative-serving.svc.cluster.local port: number: 80 weight: 100 当服务启动后，修改vs将destination指向对应服务实例上。 12345678910111213141516apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: uns route-f8c50d56-3f47-11e9-9a9a-08002715c9e6spec: hosts: - helloworld-go.default.example.com - helloworld-go.default.svc.cluster.local http: - match: route: - destination: host: helloworld-go-2xxcn-Service.default.svc.cluster.local port: number: 80 weight: 100 新版本我们创建一个简单的hello-go ksvc，并以此进行分析。ksvc如下所示： 12345678910111213apiVersion: serving.knative.dev/v1alpha1kind: Servicemetadata: name: hello-go namespace: faasspec: template: spec: containers: - image: harbor-yx-jd-dev.yx.netease.com/library/helloworld-go:v0.1 env: - name: TARGET value: &quot;Go Sample v1&quot; virtualservice的变化环境是一个标准的Istio部署，Serverless网关为Istio Ingress Gateway，所以创建完ksvc后，为了验证服务是否可以正常运行，需要发送http请求至网关。Gateway资源已经在部署Knative的时候创建，这里我们只需要关心vs。在服务副本数为0的时候，Knative控制器创建的vs关键配置如下： 12345678910111213141516171819202122232425262728spec: gateways: - knative-serving/cluster-local-gateway - knative-serving/knative-ingress-gateway hosts: - hello-go.faas - hello-go.faas.example.com - hello-go.faas.svc - hello-go.faas.svc.cluster.local - f81497077928a654cf9422088e7522d5.probe.invalid http: - match: - authority: regex: ^hello-go\\.faas\\.example\\.com(?::\\d&#123;1,5&#125;)?$ gateways: - knative-serving/knative-ingress-gateway - authority: regex: ^hello-go\\.faas(\\.svc(\\.cluster\\.local)?)?(?::\\d&#123;1,5&#125;)?$ gateways: - knative-serving/cluster-local-gateway retries: attempts: 3 perTryTimeout: 10m0s route: - destination: host: hello-go-fpmln.faas.svc.cluster.local port: number: 80 vs指定了已经创建好的gw，同时destination指向的是一个Service域名。这个Service就是Knative默认自动创建的hello-go服务的Service。可以发现vs的ownerReferences指向了一个Knative的CRD ingress.networking.internal.knative.dev： 1234567ownerReferences:- apiVersion: networking.internal.knative.dev/v1alpha1 blockOwnerDeletion: true controller: true kind: Ingress name: hello-go uid: 4a27a69e-5b9c-11ea-ae53-fa163ec7c05f 根据名字可以看到这是一个Knative内部使用的CRD，该CRD的内容其实和vs比较类似，同时ingress.networking.internal.knative.dev的ownerReferences指向了我们熟悉的route，总结下来就是：route -&gt; kingress(ingress.networking.internal.knative.dev) -&gt; vs在网关这一层涉及到的CRD资源就是如上这些。这里kingress的意义在于增加一层抽象，如果我们使用的是其他网关，则会将kingress转换成相应的网关资源配置。最新的版本中，负责kingress到Istio vs的控制器部分代码已经独立出一个项目，可见如今的Knative对Istio已经不是强依赖。现在，我们已经了解到Serverless网关是由Knative控制器最终生成的vs生效到Istio Ingress Gateway上，为了验证我们刚才部署的服务是否可以正常的运行，简单的用curl命令试验一下。 和所有的网关或者负载均衡器一样，对于7层http访问，我们需要在Header里加域名Host，用于流量转发到具体的服务。在上面的vs中已经可以看到对外域名和内部Service域名均已经配置。所以，只需要： 1curl -v -H&#x27;Host:hello-go.faas.example.com&#x27; &lt;IngressIP&gt;:&lt;Port&gt; 其中，IngressIP即网关实例对外暴露的IP。 对于冷启动来说，目前的Knative需要等十几秒，即会收到请求。根据之前老版本的经验，这个时候vs会被更新，destination指向hello-go的Service。 不过，现在我们实际发现，vs没有任何变化，仍然指向了服务的Service。对比老版本中服务副本数为0时，其实vs的destination指向的是Activator组件的。但现在，不管服务副本数如何变化，vs一直不变。 蹊跷只能从destination的Service域名入手。 revision service探索创建ksvc后，Knative会帮我们自动创建Service如下所示。 123456$ kubectl -n faas get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) hello-go ExternalName &lt;none&gt; cluster-local-gateway.istio-system.svc.cluster.local &lt;none&gt; hello-go-fpmln ClusterIP 10.178.4.126 &lt;none&gt; 80/TCP hello-go-fpmln-m9mmg ClusterIP 10.178.5.65 &lt;none&gt; 80/TCP,8022/TCP hello-go-fpmln-metrics ClusterIP 10.178.4.237 &lt;none&gt; 9090/TCP,9091/TCP hello-go Service是一个ExternalName Service，作用是将hello-go的Service域名增加一个dns CNAME别名记录，指向网关的Service域名。 根据Service的annotation我们可以发现，Knative对hello-go-fpmln、hello-go-fpmln-m9mmg 、hello-go-fpmln-metrics这三个Service的定位分别为public Service、private Service和metric Service（最新版本已经将private和metrics Service合并）。 private Service和metric Service其实不难理解。问题的关键就在这里的public Service，仔细研究hello-go-fpmln Service，我们可以发现这是一个没有labelSelector的Service，它的Endpoint不是kubernetes自动创建的，需要额外生成。 在服务副本数为0时，查看一下Service对应的Endpoint，如下所示： 12345$ kubectl -n faas get epNAME ENDPOINTS AGEhello-go-fpmln 172.31.16.81:8012 hello-go-fpmln-m9mmg 172.31.16.121:8012,172.31.16.121:8022 hello-go-fpmln-metrics 172.31.16.121:9090,172.31.16.121:9091 其中，public Service的Endpoint IP是Knative Activator的Pod IP，实际发现Activator的副本数越多这里也会相应的增加。并且由上面的分析可以看到，vs的destination指向的就是public Service。 输入几次curl命令模拟一下http请求，虽然副本数从0开始增加到1了，但是这里的Endpoint却没有变化，仍然为Activator Pod IP。 接着使用hey来压测一下： 12./hey_linux_amd64 -n 1000000 -c 300 -m GET -host helloworld-go.faas.example.com http://&lt;IngressIP&gt;:80 发现Endpoint变化了，通过对比服务的Pod IP，已经变成了新启动的服务Pod IP，不再是Activator Pod的IP。 12345$ kubectl -n faas get epNAME ENDPOINTS helloworld-go-mpk25 172.31.16.121:8012hello-go-fpmln-m9mmg 172.31.16.121:8012,172.31.16.121:8022 hello-go-fpmln-metrics 172.31.16.121:9090,172.31.16.121:9091 原来，现在新版本的冷启动流量转发机制已经不再是通过修改vs来改变网关的流量转发配置了，而是直接更新服务的public Service后端Endpoint，从而实现将流量从Activator转发到实际的服务Pod上。 通过将流量的转发功能内聚到Service/Endpoint层，一方面减小了网关的配置更新压力，一方面Knative可以在对接各种不同的网关时的实现时更加解耦，网关层不再需要关心冷启动时的流量转发机制。 流量路径再深入从上述的三个Service入手研究，它们的ownerReference是serverlessservice.networking.internal.knative.dev(sks)，而sks的ownerReference是podautoscaler.autoscaling.internal.knative.dev(kpa)。 在压测过程中同样发现，sks会在冷启动过后，会从Proxy模式变为Serve模式： 123456$ kubectl -n faas get sksNAME MODE SERVICENAME PRIVATESERVICENAME READY REASONhello-go-fpmln Proxy hello-go-fpmln hello-go-fpmln-m9mmg True$ kubectl -n faas get sksNAME MODE SERVICENAME PRIVATESERVICENAME READY REASONhello-go-fpmln Serve hello-go-fpmln hello-go-fpmln-m9mmg True 这也意味着，当流量从Activator导入的时候，sks为Proxy模式，服务真正启动起来后会变成Serve模式，网关流量直接流向服务Pod。 从名称上也可以看到，sks和kpa均为Knative内部CRD，实际上也是由于Knative设计上可以支持自定义的扩缩容方式和支持Kubernetes HPA有关，实现更高一层的抽象。","categories":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/categories/k8s/"}],"tags":[{"name":"knative","slug":"knative","permalink":"https://acodetailor.github.io/tags/knative/"}]},{"title":"Serverless之介绍","slug":"serverless","date":"2021-04-28T06:35:10.932Z","updated":"2021-05-20T01:08:34.408Z","comments":false,"path":"2021/04/28/serverless/","link":"","permalink":"https://acodetailor.github.io/2021/04/28/serverless/","excerpt":"","text":"一、Serverless是什么Serverless ，按中文翻译，称为「无服务器」。这究竟是一种什么样的形态或产品呢？无服务器，就是真的没有服务器吗？实际并不是没有服务器，只是用户只是不用更多的去考虑服务器的相关内容了，无需再去考虑服务器的规格大小、存储类型、网络带宽、自动扩缩容的问题。 二、Serverless的发展 Serverless通常是指FaaS和BaaS：Functions-as-a-Service (FaaS) ，通常提供事件驱动计算。开发人员使用由事件或HTTP请求触发的function来运行和管理应用程序代码。Backend-as-a-Service (BaaS) ，它是基于API的第三方服务，可替代应用程序中的核心功能子集。例如数据库、对象存储等第三方服务。 三、Serverless的技术特点：1、事件驱动 云函数的运行，是由事件驱动起来的，在有事件到来时，云函数会启动运行 Serverless 应用不会类似于原有的「监听 - 处理」类型的应用一直在线，而是按需启动 事件的定义可以很丰富，一次 http 请求，一个文件上传，一次数据库条目修改，一条消息发送，都可以定义为事件 2、单事件处理 云函数由事件触发，而触发启动的一个云函数实例，一次仅处理一个事件 无需在代码内考虑高并发高可靠性，代码可以专注于业务，开发更简单 通过云函数实例的高并发能力，实现业务高并发 3、自动弹性伸缩 由于云函数事件驱动及单事件处理的特性，云函数通过自动的伸缩来支持业务的高并发 针对业务的实际事件或请求数，云函数自动弹性合适的处理实例来承载实际业务量 在没有事件或请求时，无实例运行，不占用资源 4、无状态开发 云函数运行时根据业务弹性，可能伸缩到 0，无法在运行环境中保存状态数据 分布式应用开发中，均需要保持应用的无状态，以便于水平伸缩 可以利用外部服务、产品，例如数据库或缓存，实现状态数据的保存 四、业界的Serverless产品1、谷歌云 functionGoogleCloud function产品优势介绍 利用可伸缩的函数即服务 (FaaS) 运行代码，随用随付，并且无需执行任何服务器管理工作。 不必预配、管理或升级服务器 根据负载自动扩缩 集成式监控、日志记录和调试功能 基于最小权限原则的角色和函数级别的内置安全性 适用于混合云和多云端方案的关键网络功能 GoogleCloud产品界面 函数列表 创建函数，配置HTTP触发器暴露对应的函数 2、AWSAWS拥有和Google function类似的产品，叫做lambda. lambda函数列表 配置HTTP触发器暴lambda露函数 3、阿里云阿里云也有自己的Serverless产品，分别是函数计算和Serverless工作流，函数计算和Google的function，AWS的lambda基本一样，不多做介绍了。 下边看一下Serverless工作流这个产品。在Serverless工作流中，用户可以通过yaml来编排函数，事件会按照工作流的定义，流转工作流中的服务，以实现一定的业务逻辑，创建页面如下图 4、海尔智家PSI我们PSI也有一款Serverless产品，目前已上线事件驱动流程功能，配合工单通知业务，已落地生产。同步函数计算功能，正在迭代中，近期就会上线，敬请期待。 产品主界面： 事件流程界面： 事件流程通过流程图的形式编排服务，更加直观、易懂。 PSI Serverless产品体验地址：https://edgy.haier.netPSI Serverless产品文档地址：https://edgy.haier.net/doc","categories":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/tags/k8s/"}]},{"title":"java的OOM问题","slug":"java_oom","date":"2021-04-24T12:11:36.499Z","updated":"2021-04-28T06:34:43.262Z","comments":false,"path":"2021/04/24/java_oom/","link":"","permalink":"https://acodetailor.github.io/2021/04/24/java_oom/","excerpt":"","text":"一、一些概念JVM中的堆被划分为两个不同区域：新生代Young、老年代Old。新生代又划分为Eden(伊甸，标志新生)， Survivor0(s0)， Survivor(s1)。JVM中堆的GC分为：Minor GC 和 Full GC(又称为Major GC) 年轻代年轻代用于存放新创建的对象，存储大小默认为堆大小的1/15，特点是对象更替速度快，即短时间内产生大量的“死亡对象”-Xmn 可以设置年轻代为固定大小-XX:NewRatio 可以设置年轻代与老年代的大小比例年轻代使用复制-清除算法和并行收集器进行垃圾回收，对年轻代的垃圾回收称为初级回收（minor GC)minor GC 将年轻代分为三个区域，一个Eden, 两个大小相同的Survivor。应用程序只能同时使用一个Eden和一个活动Survivor, 另外一个Survivor为非活动Survivor（两个Survivor在活动与非活动间交替存在，即同一时刻只存在一个活动Survivor和一个非活动Survivor）。当发生 minor GC时：JVM执行下述操作 将程序挂起 将Eden和活动Survivor中的存活对象（存活对象指的是仍被引用的对象）复制到另一个非活动的Survivor中（记录对象被复制到另一个Survivor的次数，在此称为年龄数，每次复制+1） 清除Eden和活动Survivor中对象 将非活动Survivor标记为活动，将原来的活动Survivor标记为非活动上述即为一轮 minor GC 结束如此往复多次 minor GC后，将多次被复制的对象（高龄对象）移动到老年代中默认被移动到老年代的年龄为15，可以通过参数 -XX:MaxTenuring Threshold 来设置。当然，对于一些占用较大内存的对象，会被直接送入老年代 老年代老年代存储的是那些不会轻易“死掉”的对象，毕竟都在年轻代中熬出了头。在老年代发生的GC成为 Full GC 。Full GC 不会像 Minor GC那么频繁Full GC 采用 标记-清除算法 收集垃圾的时候会产生许多内存碎片（不连续的存储空间），因此此后若有较大的对象要进入老年代而无法找到适合的存储空间，就会提前触发一次GC收集，对内存空间进行整理 永久代永久代（Perm Gen）是JDK7中的特性，JDK8后被取消。永久区是JVM方法区的实现方式之一，JDK8起，被元空间（与堆不相连的本地空间）取而代之永久代存放的是应用元数据（应用中使用的类和方法），永久代中的对象在 Full GC 的时候进行垃圾回收 简单来讲，jvm的内存回收过程是这样的：对象在Eden Space创建，当Eden Space满了的时候，gc就把所有在Eden Space中的对象扫描一次，把所有有效的对象复制到第一个Survivor Space，同时把无效的对象所占用的空间释放。当Eden Space再次变满了的时候，就启动移动程序把Eden Space中有效的对象复制到第二个Survivor Space，同时，也将第一个Survivor Space中的有效对象复制到第二个Survivor Space。如果填充到第二个Survivor Space中的有效对象被第一个Survivor Space或Eden Space中的对象引用，那么这些对象就是长期存在的，此时这些对象将被复制到Permanent Generation。 若垃圾收集器依据这种小幅度的调整收集不能腾出足够的空间，就会运行Full GC，此时jvm gc停止所有在堆中运行的线程并执行清除动作。 二、优化配置-Xms 是指程序启动时初始内存大小（此值可以设置成与-Xmx相同，以避免每次GC完成后 JVM 内存重新分配）。默认为物理内存的1/64，最小为1M；可以指定单位，比如k、m，若不指定，则默认为字节。-Xmx 指程序运行时最大可用内存大小，程序运行中内存大于这个值会 OutOfMemory。默认为物理内存的1/4或者1G，最小为2M；单位与-Xms一致-Xmn 年轻代大小（整个JVM内存大小 = 年轻代 + 年老代 + 永久代）。-XX:NewRatio 年轻代与年老代的大小比例，-XX:NewRatio=4 设置为4，则年轻代与年老代所占比值为1：4。-XX:SurvivorRatio 年轻代中Eden区与Survivor区的大小比值，-XX:SurvivorRatio=4，设置为4，则两个Survivor区与一个Eden区的比值为 2:4-XX:MaxPermSize 设置永久代大小。-XX:MaxTenuringThreshold 设置垃圾最大年龄。如果设置为0的话，则年轻代对象不经过Survivor区，直接进入年老代。对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象再年轻代的存活时间，增加在年轻代即被回收的概论。-Xss 设置每个线程的堆栈大小。默认为512k 三、容器化场景配置使用容器内存大小，避免Cgroup的oomJDK 8u131 在 JDK 9 中有一个特性，可以在 Docker 容器运行时能够检测到多少内存的能力。在容器内部运行 JVM，它在大多数情况下将如何默认最大堆为主机内存的1/4，而非容器内存的1/4。如果对 Java 容器中的jvm虚拟机不做任何限制，当我们同时运行几个 java 容器时，很容易导致服务器的内存耗尽、负载飙升而宕机；而如果我们对容器直接进行限制，就会导致内核在某个时候杀死jvm 容器而导致频繁重启。 12345$ docker run -m 100MB openjdk:8u121 java -XshowSettings:vm -versionVM settings: Max. Heap Size (Estimated): 444.50M Ergonomics Machine Class: server Using VM: OpenJDK 64-Bit Server VM 下面我们尝试 JDK 8u131 中的实验性参数 -XX:+UseCGroupMemoryLimitForHeap 12345678$ docker run -m 100MB openjdk:8u131 java \\ -XX:+UnlockExperimentalVMOptions \\ -XX:+UseCGroupMemoryLimitForHeap \\ -XshowSettings:vm -versionVM settings: Max. Heap Size (Estimated): 44.50M Ergonomics Machine Class: server Using VM: OpenJDK 64-Bit Server VM JVM能够检测容器只有100MB，并将最大堆设置为44M。 下面尝试一个更大的容器 12345678$ docker run -m 1GB openjdk:8u131 java \\ -XX:+UnlockExperimentalVMOptions \\ -XX:+UseCGroupMemoryLimitForHeap \\ -XshowSettings:vm -versionVM settings: Max. Heap Size (Estimated): 228.00M Ergonomics Machine Class: server Using VM: OpenJDK 64-Bit Server VM 嗯，现在容器有1GB，但JVM仅使用228M作为最大堆。 除了JVM正在容器中运行以外，我们是否还可以优化它呢？ 12345678$ docker run -m 1GB openjdk:8u131 java \\ -XX:+UnlockExperimentalVMOptions \\ -XX:+UseCGroupMemoryLimitForHeap \\ -XX:MaxRAMFraction=1 -XshowSettings:vm -versionVM settings: Max. Heap Size (Estimated): 910.50M Ergonomics Machine Class: server Using VM: OpenJDK 64-Bit Server VM 使用-XX:MaxRAMFraction 我们告诉JVM使用可用内存/ MaxRAMFraction作为最大堆。使用-XX:MaxRAMFraction=1我们几乎所有可用的内存作为最大堆。","categories":[{"name":"运维","slug":"运维","permalink":"https://acodetailor.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"oom","slug":"oom","permalink":"https://acodetailor.github.io/tags/oom/"}]},{"title":"Minikube搭建istio和Knative","slug":"install knative","date":"2021-04-19T09:26:51.731Z","updated":"2021-04-28T06:32:58.096Z","comments":false,"path":"2021/04/19/install knative/","link":"","permalink":"https://acodetailor.github.io/2021/04/19/install%20knative/","excerpt":"","text":"需要搭建一套knative的测试环境，但是没有测试集群可以用了，就用自己的电脑搭建了一个测试环境。搭建过程记录一下 环境信息os:MacDriver:VirtualBox 搭建minikube一条命令即可,可以指定cpu和内存–cpus int –memory int 1minikube start --vm-driver=virtualbox --image-repository=&#x27;registry.cn-hangzhou.aliyuncs.com/google_containers&#x27; minikube常用命令 123456789101112131415161718191. minikube start 启动minikube2. minikube dashboard 打开dashboard3. minikube version 查看minikube版本4. minikube status 查看集群状态5. minikube ip 显示虚拟机ip地址6. minikube stop 停止虚拟机7. minikube ssh ssh到虚拟机中8. minikube delete 删除虚拟机9. minikube logs 查看虚拟机日志10. minikube update-check 检查更新11. minikube node list[add|start|stop|delete] 对节点进行操作12. minikube mount 将指定的目录挂载到minikube13. minikube docker-env 配置环境以使用minikube的docker守护进程14. minikube podman-env 配置环境以使用minikube的Podman服务15. minikube cache 添加，删除，或推送一个本地映像到minikube16. minikube addons 启用或禁用一个minikube插件17. minikube config 修改持久化配置值18. minikube profile 获取或者列出当前的配置文件（集群）19. minikube update-context 在IP或者端口改变的情况下更新kubeconfig 安装成功后提示如下： 12345678910😄 Darwin 10.14.6 上的 minikube v1.8.2✨ 根据现有的配置文件使用 virtualbox 驱动程序✅ 正在使用镜像存储库 registry.cn-hangzhou.aliyuncs.com/google_containers💾 Downloading preloaded images tarball for k8s v1.17.3 ...⌛ 重新配置现有主机🏃 Using the running virtualbox &quot;minikube&quot; VM ...🐳 正在 Docker 19.03.6 中准备 Kubernetes v1.17.3…🚀 正在启动 Kubernetes ... 🌟 Enabling addons: default-storageclass, storage-provisioner🏄 完成！kubectl 已经配置至 &quot;minikube&quot; 搭建istio下载istio的包，然后根据提示配置环境变量，由于某些已知原因，可以配置host 1199.232.28.133 raw.githubusercontent.com 1curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.9.1 TARGET_ARCH=x86_64 sh - 安装命令：根据需要选择合适的profile 12istioctl manifest apply --set profile=demo //使用这条命令安装失败了。istioctl install --set profile=demo -y 追加部署addons 12cd istio-1.9.1kc --context=minikube apply -f samples/addons -n istio-system 搭建knativeknative搭建比较简单，可以手动安装，再麻烦一点，可以自己下载yaml手动执行，serving和eventing一共四个yaml 1234serving-crdsserving-coreeventing-crdseventing-core 1kubectl apply --filename &quot;https://github.com/knative/serving/releases/download/v0.17.0/serving-crds.yaml&quot; 比较麻烦的是镜像，国内无法直接下载到，可以手动下载后，使用minikube cache 添加到minikube的节点上。 结果","categories":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/tags/k8s/"}]},{"title":"记一次OOM的问题排查","slug":"ops-oom","date":"2021-04-12T13:03:40.025Z","updated":"2021-04-28T06:34:43.266Z","comments":false,"path":"2021/04/12/ops-oom/","link":"","permalink":"https://acodetailor.github.io/2021/04/12/ops-oom/","excerpt":"","text":"一、问题现象 三天前，业务的pod重启了，业务想知道具体原因。 二、排查记录 首先排查kubelet日志，蛋疼的是期间业务手动重启了几次，导致不知道pod重启时所在的节点。一个个节点排查kubelet的日志（不知道后续有没有好的方法），找到节点发现日志如下： kubelet执行PLEG时发现容器挂掉了，直接给拉起了，就是业务前台发现的重启事件。 再排查业务日志，发现只有Killed,八成是OOM了。日志截图如下： 下一步排查内核日志，kerenal。发现确实是OOM了。截图如下: 然后问题来了，我们的监控发现并没有达到内存的上限，但是cgroup确实达到了上限。真是让人头大啊。 三、几个命令12zgrep &quot;&quot; *.gzjournalctl --since &#x27;2021-04-09 22:00&#x27; 四、引申问题 1、cgroup的机制 2、java的内存机制","categories":[{"name":"运维","slug":"运维","permalink":"https://acodetailor.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"oom","slug":"oom","permalink":"https://acodetailor.github.io/tags/oom/"}]},{"title":"记一次超级坑的问题排查","slug":"problem","date":"2021-04-09T06:37:26.865Z","updated":"2021-11-03T09:41:05.778Z","comments":false,"path":"2021/04/09/problem/","link":"","permalink":"https://acodetailor.github.io/2021/04/09/problem/","excerpt":"","text":"一、问题描述问题①：之前开发了一个功能，通过页面初始化gitlab代码脚手架，周一来想新建一个工程时，创建失败。gitlab仓库初始化成功，每次创建了一个文件时就失败了。 问题②：问题①解决后，线上流水线打包失败，提示net/http包要求go 1.17而我们用的是1.13. 二、排查过程问题①：本地debug，发现每次在gitlab创建文件时，第一个文件会初始化成功，但是第二个就会报400。没有其他的报错信息。考虑到gitlab刚升级了版本，所以自然的怀疑是升级引入的bug。比如新版做了限流等。1、找到gitlab服务端的容器，gitlab是裸docker单独部署的。进入容器后查看nginx的access、error日志、gitlab自己的production.log api_json.log。production.log中记录了第一次创建成功的记录，state:200。api_json.log中发现了失败的记录。state:400。 和接口一样没有任何有用的提示。2、查看gitlab的change.log也没发现相关信息。最后本地提交代码时想起来，别的组的同事给gitlab加了个钩子函数，校验commit信息的格式，比如符合”任务号 操作 信息”的格式。怀疑是这里引起的，修改代码的commit信息。run ok。 解决~问题②：问题①解决后，提交代码，流水线打包失败。提示go的net/http包引用了最新的版本，需要升级go的版本。但是本地run和build都没有问题 12&#x2F;go&#x2F;pkg&#x2F;mod&#x2F;golang.org&#x2F;x&#x2F;net@v0.0.0-20211029224645-99673261e6eb&#x2F;http2&#x2F;transport.go:417:45: undefined: os.ErrDeadlineExceedednote: module requires Go 1.17 我只改了一行字符串格式的代码，所以当时以为是流水线出了问题，看了一下打包日志，也没发现问题。让负责流水线的同事排查了。 最后反馈只有我的工程出现问题了。这就很头疼了，接着排查吧。可以发现这个出问题的包是10.29 release的，三天前思考方向：Q1：我引的包没有固定版本号，默认下载的最新的？A1：查看go.mod后发现没有。Q2：我间接引用的包没有固定版本号，导致下载了最新的包？A2：这个就很头疼了，要一个个包排查吗？ 想起使用go mod graph可以查看所有依赖关系，本地执行后发现没问题，这也和上边提到的本地没问题。Q3：github的issue看一下有没有类似的？A3：发现了几个issue和我遇到的问题一样，但是member认为不是bug，已经close了。https://github.com/golang/go/issues/45950#issuecomment-833054056、https://github.com/golang/go/issues/45946#issuecomment-832236587Q4：怀疑基础镜像被改了，导致的问题？A4：查看基础镜像历史，发现并没改。Q5：再去看看之前成功和失败的打包日志，结果发现了一个之前成功的日志里也会去拉latest的包，我以为就是我间接引用的包出了问题，一顿吐槽。然后再去打包机器上看详细的失败日志，一个可疑的日志出现了。 1go get: upgraded golang.org/x/net v0.0.0-20201110031124-69a78807bb2b =&gt; v0.0.0-20211101193420-4a448f8816b3。 为什么回去更新包版本呢。难道有go get -u ,去查看dockerfile，发现获取swag的包的时候使用了-u会去更新版本。问题语句： 1RUN go get -u github.com/swaggo/swag/cmd/swag@v1.6.7 修改后打包成功。 解决~ 三、心得问题①其实别的部门添加这个函数我是知道的，但是没有第一时间想到对自己开发的功能的影响，导致了这个问题。长时间没出问题，导致的松懈。问题②当初dockerfile看了一下没问题，直接从别的工程赋值修改的。没有对每行代码了解清楚，导致了问题。排查方向感觉也有点问题，导致时间浪费。应该先去打包机器看日志，也许解决的就快一些了。","categories":[{"name":"problem","slug":"problem","permalink":"https://acodetailor.github.io/categories/problem/"}],"tags":[{"name":"problem","slug":"problem","permalink":"https://acodetailor.github.io/tags/problem/"}]},{"title":"k8s clientSet","slug":"golang","date":"2021-04-09T06:37:13.851Z","updated":"2021-11-03T09:44:18.887Z","comments":false,"path":"2021/04/09/golang/","link":"","permalink":"https://acodetailor.github.io/2021/04/09/golang/","excerpt":"","text":"一、背景今天看了一下公司内部CMDB通过SDK暴露接口的代码，结构体十分抽象，看的我有点晕。所以想到了k8s的clientset，就想学习一下，提升一下自己的代码。Do things that are easier to understand, not easier to do。 二、分析过程1、首先要获得根据config来创建各个资源组的client，用来和apiserver交互。 123456789101112131415161718192021222324252627282930313233func NewForConfig(c *rest.Config) (*Clientset, error) &#123; configShallowCopy :&#x3D; *c if configShallowCopy.RateLimiter &#x3D;&#x3D; nil &amp;&amp; configShallowCopy.QPS &gt; 0 &#123; if configShallowCopy.Burst &lt;&#x3D; 0 &#123; return nil, fmt.Errorf(&quot;burst is required to be greater than 0 when RateLimiter is not set and QPS is set to greater than 0&quot;) &#125; configShallowCopy.RateLimiter &#x3D; flowcontrol.NewTokenBucketRateLimiter(configShallowCopy.QPS, configShallowCopy.Burst) &#125; var cs Clientset var err error cs.admissionregistrationV1, err &#x3D; admissionregistrationv1.NewForConfig(&amp;configShallowCopy) if err !&#x3D; nil &#123; return nil, err &#125; cs.admissionregistrationV1beta1, err &#x3D; admissionregistrationv1beta1.NewForConfig(&amp;configShallowCopy) if err !&#x3D; nil &#123; return nil, err &#125; cs.internalV1alpha1, err &#x3D; internalv1alpha1.NewForConfig(&amp;configShallowCopy) if err !&#x3D; nil &#123; return nil, err &#125; cs.appsV1, err &#x3D; appsv1.NewForConfig(&amp;configShallowCopy) if err !&#x3D; nil &#123; return nil, err &#125; ...&#x2F;&#x2F;重复代码，省略。 cs.DiscoveryClient, err &#x3D; discovery.NewDiscoveryClientForConfig(&amp;configShallowCopy) if err !&#x3D; nil &#123; return nil, err &#125; return &amp;cs, nil&#125; 1234567891011121314type Clientset struct &#123; *discovery.DiscoveryClient admissionregistrationV1 *admissionregistrationv1.AdmissionregistrationV1Client admissionregistrationV1beta1 *admissionregistrationv1beta1.AdmissionregistrationV1beta1Client internalV1alpha1 *internalv1alpha1.InternalV1alpha1Client appsV1 *appsv1.AppsV1Client ....&#x2F;&#x2F;各个资源client声明，省略 schedulingV1alpha1 *schedulingv1alpha1.SchedulingV1alpha1Client schedulingV1beta1 *schedulingv1beta1.SchedulingV1beta1Client schedulingV1 *schedulingv1.SchedulingV1Client storageV1beta1 *storagev1beta1.StorageV1beta1Client storageV1 *storagev1.StorageV1Client storageV1alpha1 *storagev1alpha1.StorageV1alpha1Client&#125; 2、然后我们选一个组，如appv1的，client声明如下。 1appsV1 *appsv1.AppsV1Client 对应的结构体 123type AppsV1Client struct &#123; restClient rest.Interface&#125; 结构体实现了以下方法，实现了该分组下的资源接口的管理 12345678910111213141516171819func (c *AppsV1Client) ControllerRevisions(namespace string) ControllerRevisionInterface &#123; return newControllerRevisions(c, namespace)&#125;func (c *AppsV1Client) DaemonSets(namespace string) DaemonSetInterface &#123; return newDaemonSets(c, namespace)&#125;func (c *AppsV1Client) Deployments(namespace string) DeploymentInterface &#123; return newDeployments(c, namespace)&#125;func (c *AppsV1Client) ReplicaSets(namespace string) ReplicaSetInterface &#123; return newReplicaSets(c, namespace)&#125;func (c *AppsV1Client) StatefulSets(namespace string) StatefulSetInterface &#123; return newStatefulSets(c, namespace)&#125; 以deployment为例子，DeploymentInterface包含了deployment资源的所有方法。 1234567891011121314151617type DeploymentInterface interface &#123; Create(ctx context.Context, deployment *v1.Deployment, opts metav1.CreateOptions) (*v1.Deployment, error) Update(ctx context.Context, deployment *v1.Deployment, opts metav1.UpdateOptions) (*v1.Deployment, error) UpdateStatus(ctx context.Context, deployment *v1.Deployment, opts metav1.UpdateOptions) (*v1.Deployment, error) Delete(ctx context.Context, name string, opts metav1.DeleteOptions) error DeleteCollection(ctx context.Context, opts metav1.DeleteOptions, listOpts metav1.ListOptions) error Get(ctx context.Context, name string, opts metav1.GetOptions) (*v1.Deployment, error) List(ctx context.Context, opts metav1.ListOptions) (*v1.DeploymentList, error) Watch(ctx context.Context, opts metav1.ListOptions) (watch.Interface, error) Patch(ctx context.Context, name string, pt types.PatchType, data []byte, opts metav1.PatchOptions, subresources ...string) (result *v1.Deployment, err error) Apply(ctx context.Context, deployment *appsv1.DeploymentApplyConfiguration, opts metav1.ApplyOptions) (result *v1.Deployment, err error) ApplyStatus(ctx context.Context, deployment *appsv1.DeploymentApplyConfiguration, opts metav1.ApplyOptions) (result *v1.Deployment, err error) GetScale(ctx context.Context, deploymentName string, options metav1.GetOptions) (*autoscalingv1.Scale, error) UpdateScale(ctx context.Context, deploymentName string, scale *autoscalingv1.Scale, opts metav1.UpdateOptions) (*autoscalingv1.Scale, error) DeploymentExpansion&#125; 这样就可以通过如下类似builder模式来调用，感觉还是很简洁、清楚的。 1client.AppsV1().Deployments(&quot;test&quot;).List(ctx,opts) 每次读k8s的代码都有收获！","categories":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/tags/k8s/"}]},{"title":"terraform cloud CLI","slug":"terraform_cloud","date":"2021-04-07T02:02:39.990Z","updated":"2021-04-28T06:34:43.256Z","comments":false,"path":"2021/04/07/terraform_cloud/","link":"","permalink":"https://acodetailor.github.io/2021/04/07/terraform_cloud/","excerpt":"","text":"一、简介 terraform自身提供可一个机制（类似CI/CD的流水线），可以配置自己的git库，根据git库文件内容的变化，执行plan、apply，也可以手动执行。以及配置变量文件、环境变量、state文件统一存储等等功能。 二、对比1、使用terraform二进制。 需要下载provider(terraform init) 编写tf文件，backend存储。也需要统一的配置管理。 每次手动触发。 2、使用Cloud CLI。 不需要准备provider 统一的配置管理 git push后即触发 terraform 三、栗子1、创建组织 terraform网站创建即可。 https://app.terraform.io2、配置git库 可以使用个人的git库，配置Oauth权限。根据提示 在github-&gt;setting-&gt;developer settings-&gt; oauth apps申请配置即可。 3、选择工程 git新建一个工程，然后再terraform处选择该工程。 4、测试 执行Run即可，每次修改都会触发 plan,然后需要手动执行apply。 5、结果","categories":[{"name":"terraform","slug":"terraform","permalink":"https://acodetailor.github.io/categories/terraform/"}],"tags":[{"name":"terraform","slug":"terraform","permalink":"https://acodetailor.github.io/tags/terraform/"}]},{"title":"Kubernetes CRD -- kubebuilder搭建","slug":"k8s_crd","date":"2021-04-02T02:17:18.350Z","updated":"2021-04-28T06:34:43.250Z","comments":false,"path":"2021/04/02/k8s_crd/","link":"","permalink":"https://acodetailor.github.io/2021/04/02/k8s_crd/","excerpt":"","text":"概念官方解释：CustomResourceDefinition API 资源允许你定义定制资源。 定义 CRD 对象的操作会使用你所设定的名字和模式定义（Schema）创建一个新的定制资源， Kubernetes API 负责为你的定制资源提供存储和访问服务。 CRD 对象的名称必须是合法的 DNS 子域名。 DNS子域名规则如下： 不能超过253个字符 只能包含小写字母、数字，以及’-‘ 和 ‘.’ 须以字母数字开头 须以字母数字结尾 简单来说，你可以定义像k8s原生资源如deployment、service一样定义自己的资源，而k8s会为你提供存储（ETCD）,访问（kube-apiserver）。 脚手架kubebuilder 和 operator sdk, 个人只使用过kubebuilder。 安装kubebuilder依赖1、docker2、go 建议版本在1.12以上，支持 go mod3、kustomize 代理配置配置一下 终端代理，因为初始化时会拉去go的依赖包。go 1.12之下开启 go module 12export GOPROXY=https://goproxy.ioexport GO111MODULE=on 安装一、安装kubebuilder执行如下命令。（如果curl下载失败，大概率是网络原因，可以手动下载，解压到指定目录）。 12345os= $(go env GOOS)arch=$(go env GOARCH)curl -L https://go.kubebuilder.io/dl/2.3.1/$&#123;os&#125;/$&#123;arch&#125; | tar -xz -C /tmp/sudo mv /tmp/kubebuilder_2.3.1_$&#123;os&#125;_$&#123;arch&#125; /usr/local/kubebuilderexport PATH=$PATH:/usr/local/kubebuilder/bin 二、安装kustomize 1brew install kustomize 三、安装完成后查看版本信息 12345$ kubebuilder versionVersion: version.Version&#123;KubeBuilderVersion:&quot;2.3.1&quot;, KubernetesVendor:&quot;1.16.4&quot;, GitCommit:&quot;8b53abeb4280186e494b726edf8f54ca7aa64a49&quot;, BuildDate:&quot;2020-03-26T16:42:00Z&quot;, GoOs:&quot;unknown&quot;, GoArch:&quot;unknown&quot;&#125;# sam @ MacBook-Pro-2 in ~/tf/alitest [10:45:37]$ kustomize version&#123;Version:kustomize/v3.8.2 GitCommit:e2973f6ecc9be6187cfd5ecf5e180f842249b3c6 BuildDate:2020-09-02T07:01:55+01:00 GoOs:darwin GoArch:amd64&#125; 四、初始化工程 12kubebuilder init --domain my.crd.com //初始化工程kubebuilder create api --group custom --version v1 --kind Unit //生成脚手架代码 group: 比如资源文件的apps/v1, apps即为分组，还有其他extensions、cores等。 version: 顾名思义、v1即为版本。 kind: API “顶级”资源对象的类型，每个资源对象都需要 Kind 来区分它自身代表的资源类型。比如 pod,deployment. resource: 通过 HTTP 协议以 JSON 格式发送或者读取的资源展现形式，可以以单个资源对象展现。 GVK(group、version、kind):同 Kind 不止可以出现在同一分组的不同版本中，如 apps/v1beta1 与 apps/v1，它还可能出现在不同的分组中，例如 Deployment 开始以 alpha 的特性出现在 extensions 分组，GA 之后被推进到 apps 组，所以为了严格区分不同的 Kind，需要组合 API Group、API Version 与 Kind 成为 GVK。 GVR(group、version、resource):GVR 常用于组合成 RESTful API 请求路径。例如，针对应用程序 v1 部署的 RESTful API 请求如下所示： 1GET /apis/apps/v1/namespaces/&#123;namespace&#125;/deployments/&#123;name&#125;","categories":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/tags/k8s/"}]},{"title":"Terraform介绍","slug":"terraform","date":"2021-03-30T02:38:39.714Z","updated":"2021-04-28T06:34:43.270Z","comments":false,"path":"2021/03/30/terraform/","link":"","permalink":"https://acodetailor.github.io/2021/03/30/terraform/","excerpt":"","text":"简介：Terraform是IT 基础架构自动化编排工具，它的口号是 “Write,Plan, and create Infrastructure as Code”, 基础架构即代码。 怎么理解这句话，我们先假设在没有Terraform的年代我们是怎么操作云服务。 方式一：直接登入到云平台的管控页面，人工点击按钮、键盘敲入输入参数的方式来操作，这种方式对于单个或几个云服务器还可以维护的过来，但是当云服务规模达到几十几百甚至上千以后，明显这种方式对于人力来说变得不再现实，而且容易误操作。 方式二：云平台提供了各种SDK，将对云服务的操作拆解成一个个的API供使用厂商通过代码来调用。这种方式明显好于方式一，使大批量操作变得可能，而且代码测试通过后可以避免人为误操作。但是随之带来的问题是厂商们需要专业的开发人员（Java、Python、Php、Ruby等），而且对复杂云平台的操作需要写大量的代码。 方式三：云平台提供了命令行操作云服务的工具，例如AWS CLI，这样租户厂商不再需要软件开发人员就可以实现对平台的命令操作。命令就像Sql一样，使用增删改查等操作元素来管理云。 方式四：Terraform主角登场，如果说方式三中CLI是命令式操作，需要明确的告知云服务本次操作是查询、新增、修改、还是删除，那么Terraform就是目的式操作，在本地维护了一份云服务状态的模板，模板编排成什么样子的，云服务就是什么样子的。对比方式三的优势是我们只需要专注于编排结果即可，不需要关心用什么命令去操作。 Terraform的意义在于，通过同一套规则和命令来操作不同的云平台（包括私有云）。 Terraform知识准备：核心文件有2个，一个是编排文件，一个是状态文件 main.tf文件：是业务编排的主文件，定制了一系列的编排规则，后面会有详细介绍。 terraform.tfstate：本地状态文件，相当于本地的云服务状态的备份，会影响terraform的执行计划。 如果本地状态与云服务状态不一样时会怎样？ 这个大家不需要担心，前面介绍过Terraform是目的式的编排，会按照预设结果完成编排并最终同步更新本地文件。 Provider：Terraform定制的一套接口，跟OpenStack里Dirver、Java里Interface的概念是一样的，阿里云、AWS、私有云等如果想接入进来被Terraform编排和管理就要实现一套Provider，而这些实现对于Terraform的顶层使用者来说是无感知的。 Module：可以理解为provider的集合，完成一个完整的功能。 相关命令：初始化初始化本地环境，下载provider,校验terraform版本等. 12$ terraform init //自动下载最新的provider$ terraform init -plugin-dir //指定provider目录 plan比较云端资源和本地state资源. 1$ terraform plan 部署将修改部署到云端资源. 1$ terraform apply 删除将云端资源删除. 1$ terraform destory Example创建阿里云用户组、资源组并且配置只读权限。注: alicloud : 阿里云provider名字，不能修改。 1234567891011121314151617181920212223242526provider &quot;alicloud&quot; &#123; access_key = &quot;*******************&quot; secret_key = &quot;*************************&quot; region = &quot;cn-beijing&quot; &#125; resource &quot;alicloud_ram_group&quot; &quot;group&quot; &#123; name = &quot;test_group_1000&quot; force = true &#125; resource &quot;alicloud_ram_group_policy_attachment&quot; &quot;attach&quot; &#123; policy_name = &quot;ReadOnlyAccess&quot; policy_type = &quot;System&quot; group_name = alicloud_ram_group.group.name &#125; resource &quot;alicloud_resource_manager_resource_group&quot; &quot;example&quot; &#123; resource_group_name = &quot;tftest01&quot; display_name = &quot;tftest01&quot; &#125; data &quot;alicloud_account&quot; &quot;example&quot; &#123;&#125; resource &quot;alicloud_resource_manager_policy_attachment&quot; &quot;example&quot; &#123; policy_name = &quot;ReadOnlyAccess&quot; policy_type = &quot;System&quot; principal_name = format(&quot;%s@group.%s.onaliyun.com&quot;, alicloud_ram_group.group.name, data.alicloud_account.example.id) principal_type = &quot;IMSGroup&quot; resource_group_id = alicloud_resource_manager_resource_group.example.id &#125; 其他1、配置terraform自动补全 1$ terraform -install-autocomplete 2、查看terraform的日志 12$ export TF_LOG=TRACE$ export TF_LOG_PATH=/var/log/terraform.log 3、terraform 通过配置文件或者环境变量进行配置文件目录(自己创建) 1$ $HOME/.terraformrc 环境变量 1$ os.Setenv(&quot;TF_PLUGIN_CACHE_DIR&quot;, &quot;/tmp/provider&quot;) // add provider cache state文件存储，支持consul、oss等12345678910111213terraform &#123; backend &quot;consul&quot; &#123; address = &quot;consul.example.com&quot; scheme = &quot;https&quot; path = &quot;full/path&quot; &#125;&#125;data &quot;terraform_remote_state&quot; &quot;foo&quot; &#123; backend = &quot;consul&quot; config = &#123; path = &quot;full/path&quot; &#125;&#125; 参考： https://theithollow.com/2018/05/21/using-hashicorp-consul-to-store-terraform-state/ 语法 单行注释以 # 开头 多行注释用 /* 和 */ 换行 值使用 key = value 的语法分配（空格无关紧要）。该值可以是任何原语（字符串，数字，布尔值），列表或映射。 字符串为双引号。 字符串可以使用 ${} 包装的语法对其他值进行插值，例如 ${var.foo} 。此处记录了完整的内插语法。 多行字符串可以使用外壳样式的“ here doc”语法，该字符串以类似 &lt;&lt;EOF 的标记开头，然后以 EOF 结尾。字符串和结束标志的线路必须不能缩进。 假定数字以10为底。如果为数字加上 0x 前缀，则将其视为十六进制数字。 布尔值： true ， false 。 原始类型的列表可以用方括号（ [] ）制成。示例： [“foo”, “bar”, “baz”] 。 参考 https://runebook.dev/zh-CN/docs/terraform/-index-#Alicloud https://www.terraform.io/","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://acodetailor.github.io/categories/Cloud/"}],"tags":[{"name":"terraform","slug":"terraform","permalink":"https://acodetailor.github.io/tags/terraform/"}]}],"categories":[{"name":"consul","slug":"consul","permalink":"https://acodetailor.github.io/categories/consul/"},{"name":"Java","slug":"Java","permalink":"https://acodetailor.github.io/categories/Java/"},{"name":"DB","slug":"DB","permalink":"https://acodetailor.github.io/categories/DB/"},{"name":"network","slug":"network","permalink":"https://acodetailor.github.io/categories/network/"},{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/categories/k8s/"},{"name":"运维","slug":"运维","permalink":"https://acodetailor.github.io/categories/%E8%BF%90%E7%BB%B4/"},{"name":"problem","slug":"problem","permalink":"https://acodetailor.github.io/categories/problem/"},{"name":"terraform","slug":"terraform","permalink":"https://acodetailor.github.io/categories/terraform/"},{"name":"Cloud","slug":"Cloud","permalink":"https://acodetailor.github.io/categories/Cloud/"}],"tags":[{"name":"consul","slug":"consul","permalink":"https://acodetailor.github.io/tags/consul/"},{"name":"Java","slug":"Java","permalink":"https://acodetailor.github.io/tags/Java/"},{"name":"cache","slug":"cache","permalink":"https://acodetailor.github.io/tags/cache/"},{"name":"Linux","slug":"Linux","permalink":"https://acodetailor.github.io/tags/Linux/"},{"name":"knative","slug":"knative","permalink":"https://acodetailor.github.io/tags/knative/"},{"name":"oom","slug":"oom","permalink":"https://acodetailor.github.io/tags/oom/"},{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/tags/k8s/"},{"name":"problem","slug":"problem","permalink":"https://acodetailor.github.io/tags/problem/"},{"name":"terraform","slug":"terraform","permalink":"https://acodetailor.github.io/tags/terraform/"}]}