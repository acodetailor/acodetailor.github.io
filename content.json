{"meta":{"title":"Sam","subtitle":"用我的双手，成就你的梦想。一库~","description":"杂记","author":"Sam","url":"https://acodetailor.github.io","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2021-03-30T07:24:39.640Z","updated":"2021-03-30T07:24:39.640Z","comments":false,"path":"/404.html","permalink":"https://acodetailor.github.io/404.html","excerpt":"","text":""},{"title":"书单","date":"2021-03-30T07:24:39.645Z","updated":"2021-03-30T07:24:39.644Z","comments":false,"path":"books/index.html","permalink":"https://acodetailor.github.io/books/index.html","excerpt":"","text":""},{"title":"关于","date":"2021-03-30T07:24:39.644Z","updated":"2021-03-30T07:24:39.643Z","comments":false,"path":"about/index.html","permalink":"https://acodetailor.github.io/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"分类","date":"2021-03-30T07:24:39.645Z","updated":"2021-03-30T07:24:39.645Z","comments":false,"path":"categories/index.html","permalink":"https://acodetailor.github.io/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2021-03-30T07:24:39.646Z","updated":"2021-03-30T07:24:39.646Z","comments":true,"path":"links/index.html","permalink":"https://acodetailor.github.io/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2021-03-30T07:24:39.647Z","updated":"2021-03-30T07:24:39.647Z","comments":false,"path":"repository/index.html","permalink":"https://acodetailor.github.io/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2021-03-30T07:24:39.648Z","updated":"2021-03-30T07:24:39.648Z","comments":false,"path":"tags/index.html","permalink":"https://acodetailor.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Camelk","slug":"camel","date":"2021-09-13T15:46:34.407Z","updated":"2021-09-22T03:25:42.493Z","comments":false,"path":"2021/09/13/camel/","link":"","permalink":"https://acodetailor.github.io/2021/09/13/camel/","excerpt":"","text":"一、简介Camel这个词想必很多java的开发者都很熟悉了，它是一个开源的集成框架，可以通过它快速的使用第三方的组件、调用其他系统的接口。这篇文章要介绍的camelk，其实是Camel+Knative的组合。通过CRD将Camel的组件封装成k8s集群的资源，CRD Controller创建Knative的Service资源，可以同时使用到Knative带来的弹性伸缩、事件驱动等技术优点。 二、使用下面使用一个例子来进行简单介绍。需求:通过rest接口提供数据库表的查询。实现:延伸:实现还是比较简单的，也不需要重新构建发布。延伸看来，我们可以通过这种方式快速实现数据集成、服务暴露等需求。 三、架构","categories":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/categories/k8s/"}],"tags":[{"name":"oom","slug":"oom","permalink":"https://acodetailor.github.io/tags/oom/"}]},{"title":"Serverless之Knative","slug":"serverless-istio","date":"2021-05-16T12:59:42.740Z","updated":"2021-05-16T13:48:47.655Z","comments":false,"path":"2021/05/16/serverless-istio/","link":"","permalink":"https://acodetailor.github.io/2021/05/16/serverless-istio/","excerpt":"","text":"一、什么是Knativeknative 是谷歌牵头的 serverless 架构方案，旨在提供一套简单易用的 serverless 开源方案，把 serverless 标准化和平台化。目前参与 knative 项目的公司主要有： Google、Pivotal、IBM、Red Hat和SAP。 这是 Google Cloud Platform 宣布 knative 时给出的介绍：Developed in close partnership with Pivotal, IBM, Red Hat, and SAP, Knative pushes Kubernetes-based computing forward by providing the building blocks you need to build and deploy modern, container-based serverless applications.与Pivotal，IBM，Red Hat和SAP密切合作开发，通过提供构建和部署基于容器的现代serverless应用程序所需的构建块，Knative推动基于Kubernetes的计算。 在knative的github页面，Knative 给出的官方介绍如下：Kubernetes-based platform to build, deploy, and manage modern serverless workloads.基于Kubernetes的平台，用于构建，部署和管理现代serverless工作负载。 二、Knative概述Knative扩展了Kubernetes，提供了一组中间件组件，这些组件对于构建可在任何地方运行的现代以源为中心和基于容器的应用程序至关重要：在本地，在云中，甚至在第三方数据中心。 Knative项目下的每个组件都尝试识别常见模式并编纂最佳实践，这些最佳实践被真实世界中基于Kubernetes的成功框架和应用程序共享。 Knative组件专注于解决许多平凡但困难的任务，例如： 部署容器 在Kubernetes上编排source-to-URL的工作流程 使用蓝绿部署路由和管理流量 根据需求自动收缩和调整工作负载大小 将运行服务绑定到事件生态系统 Knative的开发人员可以使用熟悉的习语，语言和框架来部署任何工作负载：函数，应用程序或容器。 组件 Build - 源到容器的构建编排 Eventing - 管理和交付事件 Serving - 请求驱动的计算，可以扩展到零 受众 开发人员Knative组件为开发人员提供Kubernetes原生API，用于将serverless风格的函数，应用程序和容器部署到自动伸缩运行时。 运维Knative组件旨在集成到更加优雅的产品中，云服务提供商或大型企业的内部团队可以随后运维。 任何企业或云提供商都可以将Knative组件应用到他们自己的系统中，并将这些收益传递给他们的客户。 Knative是一个多元化，开放和包容的社区。 三、Knative优势和已有的FaaS/serverless实现不同，knative在产品规划和设计理念上有带来新东西： 工作负载类型和标准化的 FaaS 不同，knative 期望能够运行所有的 workload : traditional application function container 针对常见应用用例提供更高级别抽象的聚焦API。 在几秒钟内即可提供可扩展，安全，无状态的服务。 松耦合特性可以按需使用组件 可插拔组件，可以自备日志和监控，网络和服务网格。Knative是可移植的：可运行于Kubernetes运行的任何地方，不用担心供应商锁定。knative 建立在 kubernetes 和 istio 之上 使用 kubernetes 提供的容器管理能力（deployment、replicaset、和 pods等），以及 istio 提供的网络管理功能（ingress、LB、dynamic route等）。 需要特别强调的是：knative 是 Kubernetes-based！ 或者说 Kubernetes-only，仅仅运行于k8s平台。","categories":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/tags/k8s/"}]},{"title":"Serverless之Knative流量转发","slug":"knative","date":"2021-04-30T01:40:25.368Z","updated":"2021-05-18T07:26:34.581Z","comments":false,"path":"2021/04/30/knative/","link":"","permalink":"https://acodetailor.github.io/2021/04/30/knative/","excerpt":"","text":"前边说过Knative是基于K8s部署的，使用k8s的容器管理功能以及istio的网络管理功能，下边一起看一下Knative的流量是怎么转发的吧。 一、Knative资源介绍Knative资源knative主要分为三个组件，build、server、event，分别处理CI/CD,服务伸缩，事件驱动。流量转发主要由server这部分处理。 knative service knative ingress knative serverlessService knative route knative configuration kantive revision istio资源 Gateway virtualService。 二、流量转发官方架构先回顾一下Knative官方的一个简单的原理示意图如下所示。用户创建一个Knative Service（ksvc）后，Knative会自动创建Route（route）、Configuration（cfg）资源，然后cfg会创建对应的Revision（rev）版本。rev实际上又会创建Deployment提供服务，流量最终会根据route的配置，导入到相应的rev中。 老版本（0.6）在集成使用Istio部署时，Route默认采用的是Istio Ingress Gateway实现，大概在Knative 0.6版本之前，我们可以发现，Route的流量转发本质上是由Istio virtualservice（vs）控制。副本数为0时，其中destination指向的是Activator组件。此时Activator会帮助转发冷启动时的请求。 12345678910111213141516171819apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: route-f8c50d56-3f47-11e9-9a9a-08002715c9e6spec: gateways: - knative-ingress-gateway - mesh hosts: - helloworld-go.default.example.com - helloworld-go.default.svc.cluster.local http: - appendHeaders: route: - destination: host: Activator-Service.knative-serving.svc.cluster.local port: number: 80 weight: 100 当服务启动后，修改vs将destination指向对应服务实例上。 12345678910111213141516apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: uns route-f8c50d56-3f47-11e9-9a9a-08002715c9e6spec: hosts: - helloworld-go.default.example.com - helloworld-go.default.svc.cluster.local http: - match: route: - destination: host: helloworld-go-2xxcn-Service.default.svc.cluster.local port: number: 80 weight: 100 新版本我们创建一个简单的hello-go ksvc，并以此进行分析。ksvc如下所示： 12345678910111213apiVersion: serving.knative.dev/v1alpha1kind: Servicemetadata: name: hello-go namespace: faasspec: template: spec: containers: - image: harbor-yx-jd-dev.yx.netease.com/library/helloworld-go:v0.1 env: - name: TARGET value: &quot;Go Sample v1&quot; virtualservice的变化环境是一个标准的Istio部署，Serverless网关为Istio Ingress Gateway，所以创建完ksvc后，为了验证服务是否可以正常运行，需要发送http请求至网关。Gateway资源已经在部署Knative的时候创建，这里我们只需要关心vs。在服务副本数为0的时候，Knative控制器创建的vs关键配置如下： 12345678910111213141516171819202122232425262728spec: gateways: - knative-serving/cluster-local-gateway - knative-serving/knative-ingress-gateway hosts: - hello-go.faas - hello-go.faas.example.com - hello-go.faas.svc - hello-go.faas.svc.cluster.local - f81497077928a654cf9422088e7522d5.probe.invalid http: - match: - authority: regex: ^hello-go\\.faas\\.example\\.com(?::\\d&#123;1,5&#125;)?$ gateways: - knative-serving/knative-ingress-gateway - authority: regex: ^hello-go\\.faas(\\.svc(\\.cluster\\.local)?)?(?::\\d&#123;1,5&#125;)?$ gateways: - knative-serving/cluster-local-gateway retries: attempts: 3 perTryTimeout: 10m0s route: - destination: host: hello-go-fpmln.faas.svc.cluster.local port: number: 80 vs指定了已经创建好的gw，同时destination指向的是一个Service域名。这个Service就是Knative默认自动创建的hello-go服务的Service。可以发现vs的ownerReferences指向了一个Knative的CRD ingress.networking.internal.knative.dev： 1234567ownerReferences:- apiVersion: networking.internal.knative.dev/v1alpha1 blockOwnerDeletion: true controller: true kind: Ingress name: hello-go uid: 4a27a69e-5b9c-11ea-ae53-fa163ec7c05f 根据名字可以看到这是一个Knative内部使用的CRD，该CRD的内容其实和vs比较类似，同时ingress.networking.internal.knative.dev的ownerReferences指向了我们熟悉的route，总结下来就是：route -&gt; kingress(ingress.networking.internal.knative.dev) -&gt; vs在网关这一层涉及到的CRD资源就是如上这些。这里kingress的意义在于增加一层抽象，如果我们使用的是其他网关，则会将kingress转换成相应的网关资源配置。最新的版本中，负责kingress到Istio vs的控制器部分代码已经独立出一个项目，可见如今的Knative对Istio已经不是强依赖。现在，我们已经了解到Serverless网关是由Knative控制器最终生成的vs生效到Istio Ingress Gateway上，为了验证我们刚才部署的服务是否可以正常的运行，简单的用curl命令试验一下。 和所有的网关或者负载均衡器一样，对于7层http访问，我们需要在Header里加域名Host，用于流量转发到具体的服务。在上面的vs中已经可以看到对外域名和内部Service域名均已经配置。所以，只需要： 1curl -v -H&#x27;Host:hello-go.faas.example.com&#x27; &lt;IngressIP&gt;:&lt;Port&gt; 其中，IngressIP即网关实例对外暴露的IP。 对于冷启动来说，目前的Knative需要等十几秒，即会收到请求。根据之前老版本的经验，这个时候vs会被更新，destination指向hello-go的Service。 不过，现在我们实际发现，vs没有任何变化，仍然指向了服务的Service。对比老版本中服务副本数为0时，其实vs的destination指向的是Activator组件的。但现在，不管服务副本数如何变化，vs一直不变。 蹊跷只能从destination的Service域名入手。 revision service探索创建ksvc后，Knative会帮我们自动创建Service如下所示。 123456$ kubectl -n faas get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) hello-go ExternalName &lt;none&gt; cluster-local-gateway.istio-system.svc.cluster.local &lt;none&gt; hello-go-fpmln ClusterIP 10.178.4.126 &lt;none&gt; 80/TCP hello-go-fpmln-m9mmg ClusterIP 10.178.5.65 &lt;none&gt; 80/TCP,8022/TCP hello-go-fpmln-metrics ClusterIP 10.178.4.237 &lt;none&gt; 9090/TCP,9091/TCP hello-go Service是一个ExternalName Service，作用是将hello-go的Service域名增加一个dns CNAME别名记录，指向网关的Service域名。 根据Service的annotation我们可以发现，Knative对hello-go-fpmln、hello-go-fpmln-m9mmg 、hello-go-fpmln-metrics这三个Service的定位分别为public Service、private Service和metric Service（最新版本已经将private和metrics Service合并）。 private Service和metric Service其实不难理解。问题的关键就在这里的public Service，仔细研究hello-go-fpmln Service，我们可以发现这是一个没有labelSelector的Service，它的Endpoint不是kubernetes自动创建的，需要额外生成。 在服务副本数为0时，查看一下Service对应的Endpoint，如下所示： 12345$ kubectl -n faas get epNAME ENDPOINTS AGEhello-go-fpmln 172.31.16.81:8012 hello-go-fpmln-m9mmg 172.31.16.121:8012,172.31.16.121:8022 hello-go-fpmln-metrics 172.31.16.121:9090,172.31.16.121:9091 其中，public Service的Endpoint IP是Knative Activator的Pod IP，实际发现Activator的副本数越多这里也会相应的增加。并且由上面的分析可以看到，vs的destination指向的就是public Service。 输入几次curl命令模拟一下http请求，虽然副本数从0开始增加到1了，但是这里的Endpoint却没有变化，仍然为Activator Pod IP。 接着使用hey来压测一下： 12./hey_linux_amd64 -n 1000000 -c 300 -m GET -host helloworld-go.faas.example.com http://&lt;IngressIP&gt;:80 发现Endpoint变化了，通过对比服务的Pod IP，已经变成了新启动的服务Pod IP，不再是Activator Pod的IP。 12345$ kubectl -n faas get epNAME ENDPOINTS helloworld-go-mpk25 172.31.16.121:8012hello-go-fpmln-m9mmg 172.31.16.121:8012,172.31.16.121:8022 hello-go-fpmln-metrics 172.31.16.121:9090,172.31.16.121:9091 原来，现在新版本的冷启动流量转发机制已经不再是通过修改vs来改变网关的流量转发配置了，而是直接更新服务的public Service后端Endpoint，从而实现将流量从Activator转发到实际的服务Pod上。 通过将流量的转发功能内聚到Service/Endpoint层，一方面减小了网关的配置更新压力，一方面Knative可以在对接各种不同的网关时的实现时更加解耦，网关层不再需要关心冷启动时的流量转发机制。 流量路径再深入从上述的三个Service入手研究，它们的ownerReference是serverlessservice.networking.internal.knative.dev(sks)，而sks的ownerReference是podautoscaler.autoscaling.internal.knative.dev(kpa)。 在压测过程中同样发现，sks会在冷启动过后，会从Proxy模式变为Serve模式： 123456$ kubectl -n faas get sksNAME MODE SERVICENAME PRIVATESERVICENAME READY REASONhello-go-fpmln Proxy hello-go-fpmln hello-go-fpmln-m9mmg True$ kubectl -n faas get sksNAME MODE SERVICENAME PRIVATESERVICENAME READY REASONhello-go-fpmln Serve hello-go-fpmln hello-go-fpmln-m9mmg True 这也意味着，当流量从Activator导入的时候，sks为Proxy模式，服务真正启动起来后会变成Serve模式，网关流量直接流向服务Pod。 从名称上也可以看到，sks和kpa均为Knative内部CRD，实际上也是由于Knative设计上可以支持自定义的扩缩容方式和支持Kubernetes HPA有关，实现更高一层的抽象。","categories":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/categories/k8s/"}],"tags":[{"name":"knative","slug":"knative","permalink":"https://acodetailor.github.io/tags/knative/"}]},{"title":"Serverless之介绍","slug":"serverless","date":"2021-04-28T06:35:10.932Z","updated":"2021-05-20T01:08:34.408Z","comments":false,"path":"2021/04/28/serverless/","link":"","permalink":"https://acodetailor.github.io/2021/04/28/serverless/","excerpt":"","text":"一、Serverless是什么Serverless ，按中文翻译，称为「无服务器」。这究竟是一种什么样的形态或产品呢？无服务器，就是真的没有服务器吗？实际并不是没有服务器，只是用户只是不用更多的去考虑服务器的相关内容了，无需再去考虑服务器的规格大小、存储类型、网络带宽、自动扩缩容的问题。 二、Serverless的发展 Serverless通常是指FaaS和BaaS：Functions-as-a-Service (FaaS) ，通常提供事件驱动计算。开发人员使用由事件或HTTP请求触发的function来运行和管理应用程序代码。Backend-as-a-Service (BaaS) ，它是基于API的第三方服务，可替代应用程序中的核心功能子集。例如数据库、对象存储等第三方服务。 三、Serverless的技术特点：1、事件驱动 云函数的运行，是由事件驱动起来的，在有事件到来时，云函数会启动运行 Serverless 应用不会类似于原有的「监听 - 处理」类型的应用一直在线，而是按需启动 事件的定义可以很丰富，一次 http 请求，一个文件上传，一次数据库条目修改，一条消息发送，都可以定义为事件 2、单事件处理 云函数由事件触发，而触发启动的一个云函数实例，一次仅处理一个事件 无需在代码内考虑高并发高可靠性，代码可以专注于业务，开发更简单 通过云函数实例的高并发能力，实现业务高并发 3、自动弹性伸缩 由于云函数事件驱动及单事件处理的特性，云函数通过自动的伸缩来支持业务的高并发 针对业务的实际事件或请求数，云函数自动弹性合适的处理实例来承载实际业务量 在没有事件或请求时，无实例运行，不占用资源 4、无状态开发 云函数运行时根据业务弹性，可能伸缩到 0，无法在运行环境中保存状态数据 分布式应用开发中，均需要保持应用的无状态，以便于水平伸缩 可以利用外部服务、产品，例如数据库或缓存，实现状态数据的保存 四、业界的Serverless产品1、谷歌云 functionGoogleCloud function产品优势介绍 利用可伸缩的函数即服务 (FaaS) 运行代码，随用随付，并且无需执行任何服务器管理工作。 不必预配、管理或升级服务器 根据负载自动扩缩 集成式监控、日志记录和调试功能 基于最小权限原则的角色和函数级别的内置安全性 适用于混合云和多云端方案的关键网络功能 GoogleCloud产品界面 函数列表 创建函数，配置HTTP触发器暴露对应的函数 2、AWSAWS拥有和Google function类似的产品，叫做lambda. lambda函数列表 配置HTTP触发器暴lambda露函数 3、阿里云阿里云也有自己的Serverless产品，分别是函数计算和Serverless工作流，函数计算和Google的function，AWS的lambda基本一样，不多做介绍了。 下边看一下Serverless工作流这个产品。在Serverless工作流中，用户可以通过yaml来编排函数，事件会按照工作流的定义，流转工作流中的服务，以实现一定的业务逻辑，创建页面如下图 4、海尔智家PSI我们PSI也有一款Serverless产品，目前已上线事件驱动流程功能，配合工单通知业务，已落地生产。同步函数计算功能，正在迭代中，近期就会上线，敬请期待。 产品主界面： 事件流程界面： 事件流程通过流程图的形式编排服务，更加直观、易懂。 PSI Serverless产品体验地址：https://edgy.haier.netPSI Serverless产品文档地址：https://edgy.haier.net/doc","categories":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/tags/k8s/"}]},{"title":"java的OOM问题","slug":"java_oom","date":"2021-04-24T12:11:36.499Z","updated":"2021-04-28T06:34:43.262Z","comments":false,"path":"2021/04/24/java_oom/","link":"","permalink":"https://acodetailor.github.io/2021/04/24/java_oom/","excerpt":"","text":"一、一些概念JVM中的堆被划分为两个不同区域：新生代Young、老年代Old。新生代又划分为Eden(伊甸，标志新生)， Survivor0(s0)， Survivor(s1)。JVM中堆的GC分为：Minor GC 和 Full GC(又称为Major GC) 年轻代年轻代用于存放新创建的对象，存储大小默认为堆大小的1/15，特点是对象更替速度快，即短时间内产生大量的“死亡对象”-Xmn 可以设置年轻代为固定大小-XX:NewRatio 可以设置年轻代与老年代的大小比例年轻代使用复制-清除算法和并行收集器进行垃圾回收，对年轻代的垃圾回收称为初级回收（minor GC)minor GC 将年轻代分为三个区域，一个Eden, 两个大小相同的Survivor。应用程序只能同时使用一个Eden和一个活动Survivor, 另外一个Survivor为非活动Survivor（两个Survivor在活动与非活动间交替存在，即同一时刻只存在一个活动Survivor和一个非活动Survivor）。当发生 minor GC时：JVM执行下述操作 将程序挂起 将Eden和活动Survivor中的存活对象（存活对象指的是仍被引用的对象）复制到另一个非活动的Survivor中（记录对象被复制到另一个Survivor的次数，在此称为年龄数，每次复制+1） 清除Eden和活动Survivor中对象 将非活动Survivor标记为活动，将原来的活动Survivor标记为非活动上述即为一轮 minor GC 结束如此往复多次 minor GC后，将多次被复制的对象（高龄对象）移动到老年代中默认被移动到老年代的年龄为15，可以通过参数 -XX:MaxTenuring Threshold 来设置。当然，对于一些占用较大内存的对象，会被直接送入老年代 老年代老年代存储的是那些不会轻易“死掉”的对象，毕竟都在年轻代中熬出了头。在老年代发生的GC成为 Full GC 。Full GC 不会像 Minor GC那么频繁Full GC 采用 标记-清除算法 收集垃圾的时候会产生许多内存碎片（不连续的存储空间），因此此后若有较大的对象要进入老年代而无法找到适合的存储空间，就会提前触发一次GC收集，对内存空间进行整理 永久代永久代（Perm Gen）是JDK7中的特性，JDK8后被取消。永久区是JVM方法区的实现方式之一，JDK8起，被元空间（与堆不相连的本地空间）取而代之永久代存放的是应用元数据（应用中使用的类和方法），永久代中的对象在 Full GC 的时候进行垃圾回收 简单来讲，jvm的内存回收过程是这样的：对象在Eden Space创建，当Eden Space满了的时候，gc就把所有在Eden Space中的对象扫描一次，把所有有效的对象复制到第一个Survivor Space，同时把无效的对象所占用的空间释放。当Eden Space再次变满了的时候，就启动移动程序把Eden Space中有效的对象复制到第二个Survivor Space，同时，也将第一个Survivor Space中的有效对象复制到第二个Survivor Space。如果填充到第二个Survivor Space中的有效对象被第一个Survivor Space或Eden Space中的对象引用，那么这些对象就是长期存在的，此时这些对象将被复制到Permanent Generation。 若垃圾收集器依据这种小幅度的调整收集不能腾出足够的空间，就会运行Full GC，此时jvm gc停止所有在堆中运行的线程并执行清除动作。 二、优化配置-Xms 是指程序启动时初始内存大小（此值可以设置成与-Xmx相同，以避免每次GC完成后 JVM 内存重新分配）。默认为物理内存的1/64，最小为1M；可以指定单位，比如k、m，若不指定，则默认为字节。-Xmx 指程序运行时最大可用内存大小，程序运行中内存大于这个值会 OutOfMemory。默认为物理内存的1/4或者1G，最小为2M；单位与-Xms一致-Xmn 年轻代大小（整个JVM内存大小 = 年轻代 + 年老代 + 永久代）。-XX:NewRatio 年轻代与年老代的大小比例，-XX:NewRatio=4 设置为4，则年轻代与年老代所占比值为1：4。-XX:SurvivorRatio 年轻代中Eden区与Survivor区的大小比值，-XX:SurvivorRatio=4，设置为4，则两个Survivor区与一个Eden区的比值为 2:4-XX:MaxPermSize 设置永久代大小。-XX:MaxTenuringThreshold 设置垃圾最大年龄。如果设置为0的话，则年轻代对象不经过Survivor区，直接进入年老代。对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象再年轻代的存活时间，增加在年轻代即被回收的概论。-Xss 设置每个线程的堆栈大小。默认为512k 三、容器化场景配置使用容器内存大小，避免Cgroup的oomJDK 8u131 在 JDK 9 中有一个特性，可以在 Docker 容器运行时能够检测到多少内存的能力。在容器内部运行 JVM，它在大多数情况下将如何默认最大堆为主机内存的1/4，而非容器内存的1/4。如果对 Java 容器中的jvm虚拟机不做任何限制，当我们同时运行几个 java 容器时，很容易导致服务器的内存耗尽、负载飙升而宕机；而如果我们对容器直接进行限制，就会导致内核在某个时候杀死jvm 容器而导致频繁重启。 12345$ docker run -m 100MB openjdk:8u121 java -XshowSettings:vm -versionVM settings: Max. Heap Size (Estimated): 444.50M Ergonomics Machine Class: server Using VM: OpenJDK 64-Bit Server VM 下面我们尝试 JDK 8u131 中的实验性参数 -XX:+UseCGroupMemoryLimitForHeap 12345678$ docker run -m 100MB openjdk:8u131 java \\ -XX:+UnlockExperimentalVMOptions \\ -XX:+UseCGroupMemoryLimitForHeap \\ -XshowSettings:vm -versionVM settings: Max. Heap Size (Estimated): 44.50M Ergonomics Machine Class: server Using VM: OpenJDK 64-Bit Server VM JVM能够检测容器只有100MB，并将最大堆设置为44M。 下面尝试一个更大的容器 12345678$ docker run -m 1GB openjdk:8u131 java \\ -XX:+UnlockExperimentalVMOptions \\ -XX:+UseCGroupMemoryLimitForHeap \\ -XshowSettings:vm -versionVM settings: Max. Heap Size (Estimated): 228.00M Ergonomics Machine Class: server Using VM: OpenJDK 64-Bit Server VM 嗯，现在容器有1GB，但JVM仅使用228M作为最大堆。 除了JVM正在容器中运行以外，我们是否还可以优化它呢？ 12345678$ docker run -m 1GB openjdk:8u131 java \\ -XX:+UnlockExperimentalVMOptions \\ -XX:+UseCGroupMemoryLimitForHeap \\ -XX:MaxRAMFraction=1 -XshowSettings:vm -versionVM settings: Max. Heap Size (Estimated): 910.50M Ergonomics Machine Class: server Using VM: OpenJDK 64-Bit Server VM 使用-XX:MaxRAMFraction 我们告诉JVM使用可用内存/ MaxRAMFraction作为最大堆。使用-XX:MaxRAMFraction=1我们几乎所有可用的内存作为最大堆。","categories":[{"name":"运维","slug":"运维","permalink":"https://acodetailor.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"oom","slug":"oom","permalink":"https://acodetailor.github.io/tags/oom/"}]},{"title":"Minikube搭建istio和Knative","slug":"install knative","date":"2021-04-19T09:26:51.731Z","updated":"2021-04-28T06:32:58.096Z","comments":false,"path":"2021/04/19/install knative/","link":"","permalink":"https://acodetailor.github.io/2021/04/19/install%20knative/","excerpt":"","text":"需要搭建一套knative的测试环境，但是没有测试集群可以用了，就用自己的电脑搭建了一个测试环境。搭建过程记录一下 环境信息os:MacDriver:VirtualBox 搭建minikube一条命令即可,可以指定cpu和内存–cpus int –memory int 1minikube start --vm-driver=virtualbox --image-repository=&#x27;registry.cn-hangzhou.aliyuncs.com/google_containers&#x27; minikube常用命令 123456789101112131415161718191. minikube start 启动minikube2. minikube dashboard 打开dashboard3. minikube version 查看minikube版本4. minikube status 查看集群状态5. minikube ip 显示虚拟机ip地址6. minikube stop 停止虚拟机7. minikube ssh ssh到虚拟机中8. minikube delete 删除虚拟机9. minikube logs 查看虚拟机日志10. minikube update-check 检查更新11. minikube node list[add|start|stop|delete] 对节点进行操作12. minikube mount 将指定的目录挂载到minikube13. minikube docker-env 配置环境以使用minikube的docker守护进程14. minikube podman-env 配置环境以使用minikube的Podman服务15. minikube cache 添加，删除，或推送一个本地映像到minikube16. minikube addons 启用或禁用一个minikube插件17. minikube config 修改持久化配置值18. minikube profile 获取或者列出当前的配置文件（集群）19. minikube update-context 在IP或者端口改变的情况下更新kubeconfig 安装成功后提示如下： 12345678910😄 Darwin 10.14.6 上的 minikube v1.8.2✨ 根据现有的配置文件使用 virtualbox 驱动程序✅ 正在使用镜像存储库 registry.cn-hangzhou.aliyuncs.com/google_containers💾 Downloading preloaded images tarball for k8s v1.17.3 ...⌛ 重新配置现有主机🏃 Using the running virtualbox &quot;minikube&quot; VM ...🐳 正在 Docker 19.03.6 中准备 Kubernetes v1.17.3…🚀 正在启动 Kubernetes ... 🌟 Enabling addons: default-storageclass, storage-provisioner🏄 完成！kubectl 已经配置至 &quot;minikube&quot; 搭建istio下载istio的包，然后根据提示配置环境变量，由于某些已知原因，可以配置host 1199.232.28.133 raw.githubusercontent.com 1curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.9.1 TARGET_ARCH=x86_64 sh - 安装命令：根据需要选择合适的profile 12istioctl manifest apply --set profile=demo //使用这条命令安装失败了。istioctl install --set profile=demo -y 追加部署addons 12cd istio-1.9.1kc --context=minikube apply -f samples/addons -n istio-system 搭建knativeknative搭建比较简单，可以手动安装，再麻烦一点，可以自己下载yaml手动执行，serving和eventing一共四个yaml 1234serving-crdsserving-coreeventing-crdseventing-core 1kubectl apply --filename &quot;https://github.com/knative/serving/releases/download/v0.17.0/serving-crds.yaml&quot; 比较麻烦的是镜像，国内无法直接下载到，可以手动下载后，使用minikube cache 添加到minikube的节点上。 结果","categories":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/tags/k8s/"}]},{"title":"记一次OOM的问题排查","slug":"ops-oom","date":"2021-04-12T13:03:40.025Z","updated":"2021-04-28T06:34:43.266Z","comments":false,"path":"2021/04/12/ops-oom/","link":"","permalink":"https://acodetailor.github.io/2021/04/12/ops-oom/","excerpt":"","text":"一、问题现象 三天前，业务的pod重启了，业务想知道具体原因。 二、排查记录 首先排查kubelet日志，蛋疼的是期间业务手动重启了几次，导致不知道pod重启时所在的节点。一个个节点排查kubelet的日志（不知道后续有没有好的方法），找到节点发现日志如下： kubelet执行PLEG时发现容器挂掉了，直接给拉起了，就是业务前台发现的重启事件。 再排查业务日志，发现只有Killed,八成是OOM了。日志截图如下： 下一步排查内核日志，kerenal。发现确实是OOM了。截图如下: 然后问题来了，我们的监控发现并没有达到内存的上限，但是cgroup确实达到了上限。真是让人头大啊。 三、几个命令12zgrep &quot;&quot; *.gzjournalctl --since &#x27;2021-04-09 22:00&#x27; 四、引申问题 1、cgroup的机制 2、java的内存机制","categories":[{"name":"运维","slug":"运维","permalink":"https://acodetailor.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"oom","slug":"oom","permalink":"https://acodetailor.github.io/tags/oom/"}]},{"title":"","slug":"TCP","date":"2021-04-09T06:37:26.865Z","updated":"2021-04-09T06:37:26.865Z","comments":true,"path":"2021/04/09/TCP/","link":"","permalink":"https://acodetailor.github.io/2021/04/09/TCP/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"","slug":"Iptabels And Ipvs","date":"2021-04-09T06:37:13.851Z","updated":"2021-04-09T06:37:13.851Z","comments":true,"path":"2021/04/09/Iptabels And Ipvs/","link":"","permalink":"https://acodetailor.github.io/2021/04/09/Iptabels%20And%20Ipvs/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"terraform cloud CLI","slug":"terraform_cloud","date":"2021-04-07T02:02:39.990Z","updated":"2021-04-28T06:34:43.256Z","comments":false,"path":"2021/04/07/terraform_cloud/","link":"","permalink":"https://acodetailor.github.io/2021/04/07/terraform_cloud/","excerpt":"","text":"一、简介 terraform自身提供可一个机制（类似CI/CD的流水线），可以配置自己的git库，根据git库文件内容的变化，执行plan、apply，也可以手动执行。以及配置变量文件、环境变量、state文件统一存储等等功能。 二、对比1、使用terraform二进制。 需要下载provider(terraform init) 编写tf文件，backend存储。也需要统一的配置管理。 每次手动触发。 2、使用Cloud CLI。 不需要准备provider 统一的配置管理 git push后即触发 terraform 三、栗子1、创建组织 terraform网站创建即可。 https://app.terraform.io2、配置git库 可以使用个人的git库，配置Oauth权限。根据提示 在github-&gt;setting-&gt;developer settings-&gt; oauth apps申请配置即可。 3、选择工程 git新建一个工程，然后再terraform处选择该工程。 4、测试 执行Run即可，每次修改都会触发 plan,然后需要手动执行apply。 5、结果","categories":[{"name":"terraform","slug":"terraform","permalink":"https://acodetailor.github.io/categories/terraform/"}],"tags":[{"name":"terraform","slug":"terraform","permalink":"https://acodetailor.github.io/tags/terraform/"}]},{"title":"Kubernetes CRD -- kubebuilder搭建","slug":"k8s_crd","date":"2021-04-02T02:17:18.350Z","updated":"2021-04-28T06:34:43.250Z","comments":false,"path":"2021/04/02/k8s_crd/","link":"","permalink":"https://acodetailor.github.io/2021/04/02/k8s_crd/","excerpt":"","text":"概念官方解释：CustomResourceDefinition API 资源允许你定义定制资源。 定义 CRD 对象的操作会使用你所设定的名字和模式定义（Schema）创建一个新的定制资源， Kubernetes API 负责为你的定制资源提供存储和访问服务。 CRD 对象的名称必须是合法的 DNS 子域名。 DNS子域名规则如下： 不能超过253个字符 只能包含小写字母、数字，以及’-‘ 和 ‘.’ 须以字母数字开头 须以字母数字结尾 简单来说，你可以定义像k8s原生资源如deployment、service一样定义自己的资源，而k8s会为你提供存储（ETCD）,访问（kube-apiserver）。 脚手架kubebuilder 和 operator sdk, 个人只使用过kubebuilder。 安装kubebuilder依赖1、docker2、go 建议版本在1.12以上，支持 go mod3、kustomize 代理配置配置一下 终端代理，因为初始化时会拉去go的依赖包。go 1.12之下开启 go module 12export GOPROXY=https://goproxy.ioexport GO111MODULE=on 安装一、安装kubebuilder执行如下命令。（如果curl下载失败，大概率是网络原因，可以手动下载，解压到指定目录）。 12345os= $(go env GOOS)arch=$(go env GOARCH)curl -L https://go.kubebuilder.io/dl/2.3.1/$&#123;os&#125;/$&#123;arch&#125; | tar -xz -C /tmp/sudo mv /tmp/kubebuilder_2.3.1_$&#123;os&#125;_$&#123;arch&#125; /usr/local/kubebuilderexport PATH=$PATH:/usr/local/kubebuilder/bin 二、安装kustomize 1brew install kustomize 三、安装完成后查看版本信息 12345$ kubebuilder versionVersion: version.Version&#123;KubeBuilderVersion:&quot;2.3.1&quot;, KubernetesVendor:&quot;1.16.4&quot;, GitCommit:&quot;8b53abeb4280186e494b726edf8f54ca7aa64a49&quot;, BuildDate:&quot;2020-03-26T16:42:00Z&quot;, GoOs:&quot;unknown&quot;, GoArch:&quot;unknown&quot;&#125;# sam @ MacBook-Pro-2 in ~/tf/alitest [10:45:37]$ kustomize version&#123;Version:kustomize/v3.8.2 GitCommit:e2973f6ecc9be6187cfd5ecf5e180f842249b3c6 BuildDate:2020-09-02T07:01:55+01:00 GoOs:darwin GoArch:amd64&#125; 四、初始化工程 12kubebuilder init --domain my.crd.com //初始化工程kubebuilder create api --group custom --version v1 --kind Unit //生成脚手架代码 group: 比如资源文件的apps/v1, apps即为分组，还有其他extensions、cores等。 version: 顾名思义、v1即为版本。 kind: API “顶级”资源对象的类型，每个资源对象都需要 Kind 来区分它自身代表的资源类型。比如 pod,deployment. resource: 通过 HTTP 协议以 JSON 格式发送或者读取的资源展现形式，可以以单个资源对象展现。 GVK(group、version、kind):同 Kind 不止可以出现在同一分组的不同版本中，如 apps/v1beta1 与 apps/v1，它还可能出现在不同的分组中，例如 Deployment 开始以 alpha 的特性出现在 extensions 分组，GA 之后被推进到 apps 组，所以为了严格区分不同的 Kind，需要组合 API Group、API Version 与 Kind 成为 GVK。 GVR(group、version、resource):GVR 常用于组合成 RESTful API 请求路径。例如，针对应用程序 v1 部署的 RESTful API 请求如下所示： 1GET /apis/apps/v1/namespaces/&#123;namespace&#125;/deployments/&#123;name&#125;","categories":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/tags/k8s/"}]},{"title":"Terraform介绍","slug":"terraform","date":"2021-03-30T02:38:39.714Z","updated":"2021-04-28T06:34:43.270Z","comments":false,"path":"2021/03/30/terraform/","link":"","permalink":"https://acodetailor.github.io/2021/03/30/terraform/","excerpt":"","text":"简介：Terraform是IT 基础架构自动化编排工具，它的口号是 “Write,Plan, and create Infrastructure as Code”, 基础架构即代码。 怎么理解这句话，我们先假设在没有Terraform的年代我们是怎么操作云服务。 方式一：直接登入到云平台的管控页面，人工点击按钮、键盘敲入输入参数的方式来操作，这种方式对于单个或几个云服务器还可以维护的过来，但是当云服务规模达到几十几百甚至上千以后，明显这种方式对于人力来说变得不再现实，而且容易误操作。 方式二：云平台提供了各种SDK，将对云服务的操作拆解成一个个的API供使用厂商通过代码来调用。这种方式明显好于方式一，使大批量操作变得可能，而且代码测试通过后可以避免人为误操作。但是随之带来的问题是厂商们需要专业的开发人员（Java、Python、Php、Ruby等），而且对复杂云平台的操作需要写大量的代码。 方式三：云平台提供了命令行操作云服务的工具，例如AWS CLI，这样租户厂商不再需要软件开发人员就可以实现对平台的命令操作。命令就像Sql一样，使用增删改查等操作元素来管理云。 方式四：Terraform主角登场，如果说方式三中CLI是命令式操作，需要明确的告知云服务本次操作是查询、新增、修改、还是删除，那么Terraform就是目的式操作，在本地维护了一份云服务状态的模板，模板编排成什么样子的，云服务就是什么样子的。对比方式三的优势是我们只需要专注于编排结果即可，不需要关心用什么命令去操作。 Terraform的意义在于，通过同一套规则和命令来操作不同的云平台（包括私有云）。 Terraform知识准备：核心文件有2个，一个是编排文件，一个是状态文件 main.tf文件：是业务编排的主文件，定制了一系列的编排规则，后面会有详细介绍。 terraform.tfstate：本地状态文件，相当于本地的云服务状态的备份，会影响terraform的执行计划。 如果本地状态与云服务状态不一样时会怎样？ 这个大家不需要担心，前面介绍过Terraform是目的式的编排，会按照预设结果完成编排并最终同步更新本地文件。 Provider：Terraform定制的一套接口，跟OpenStack里Dirver、Java里Interface的概念是一样的，阿里云、AWS、私有云等如果想接入进来被Terraform编排和管理就要实现一套Provider，而这些实现对于Terraform的顶层使用者来说是无感知的。 Module：可以理解为provider的集合，完成一个完整的功能。 相关命令：初始化初始化本地环境，下载provider,校验terraform版本等. 12$ terraform init //自动下载最新的provider$ terraform init -plugin-dir //指定provider目录 plan比较云端资源和本地state资源. 1$ terraform plan 部署将修改部署到云端资源. 1$ terraform apply 删除将云端资源删除. 1$ terraform destory Example创建阿里云用户组、资源组并且配置只读权限。注: alicloud : 阿里云provider名字，不能修改。 1234567891011121314151617181920212223242526provider &quot;alicloud&quot; &#123; access_key = &quot;*******************&quot; secret_key = &quot;*************************&quot; region = &quot;cn-beijing&quot; &#125; resource &quot;alicloud_ram_group&quot; &quot;group&quot; &#123; name = &quot;test_group_1000&quot; force = true &#125; resource &quot;alicloud_ram_group_policy_attachment&quot; &quot;attach&quot; &#123; policy_name = &quot;ReadOnlyAccess&quot; policy_type = &quot;System&quot; group_name = alicloud_ram_group.group.name &#125; resource &quot;alicloud_resource_manager_resource_group&quot; &quot;example&quot; &#123; resource_group_name = &quot;tftest01&quot; display_name = &quot;tftest01&quot; &#125; data &quot;alicloud_account&quot; &quot;example&quot; &#123;&#125; resource &quot;alicloud_resource_manager_policy_attachment&quot; &quot;example&quot; &#123; policy_name = &quot;ReadOnlyAccess&quot; policy_type = &quot;System&quot; principal_name = format(&quot;%s@group.%s.onaliyun.com&quot;, alicloud_ram_group.group.name, data.alicloud_account.example.id) principal_type = &quot;IMSGroup&quot; resource_group_id = alicloud_resource_manager_resource_group.example.id &#125; 其他1、配置terraform自动补全 1$ terraform -install-autocomplete 2、查看terraform的日志 12$ export TF_LOG=TRACE$ export TF_LOG_PATH=/var/log/terraform.log 3、terraform 通过配置文件或者环境变量进行配置文件目录(自己创建) 1$ $HOME/.terraformrc 环境变量 1$ os.Setenv(&quot;TF_PLUGIN_CACHE_DIR&quot;, &quot;/tmp/provider&quot;) // add provider cache state文件存储，支持consul、oss等12345678910111213terraform &#123; backend &quot;consul&quot; &#123; address = &quot;consul.example.com&quot; scheme = &quot;https&quot; path = &quot;full/path&quot; &#125;&#125;data &quot;terraform_remote_state&quot; &quot;foo&quot; &#123; backend = &quot;consul&quot; config = &#123; path = &quot;full/path&quot; &#125;&#125; 参考： https://theithollow.com/2018/05/21/using-hashicorp-consul-to-store-terraform-state/ 语法 单行注释以 # 开头 多行注释用 /* 和 */ 换行 值使用 key = value 的语法分配（空格无关紧要）。该值可以是任何原语（字符串，数字，布尔值），列表或映射。 字符串为双引号。 字符串可以使用 ${} 包装的语法对其他值进行插值，例如 ${var.foo} 。此处记录了完整的内插语法。 多行字符串可以使用外壳样式的“ here doc”语法，该字符串以类似 &lt;&lt;EOF 的标记开头，然后以 EOF 结尾。字符串和结束标志的线路必须不能缩进。 假定数字以10为底。如果为数字加上 0x 前缀，则将其视为十六进制数字。 布尔值： true ， false 。 原始类型的列表可以用方括号（ [] ）制成。示例： [“foo”, “bar”, “baz”] 。 参考 https://runebook.dev/zh-CN/docs/terraform/-index-#Alicloud https://www.terraform.io/","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://acodetailor.github.io/categories/Cloud/"}],"tags":[{"name":"terraform","slug":"terraform","permalink":"https://acodetailor.github.io/tags/terraform/"}]}],"categories":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/categories/k8s/"},{"name":"运维","slug":"运维","permalink":"https://acodetailor.github.io/categories/%E8%BF%90%E7%BB%B4/"},{"name":"terraform","slug":"terraform","permalink":"https://acodetailor.github.io/categories/terraform/"},{"name":"Cloud","slug":"Cloud","permalink":"https://acodetailor.github.io/categories/Cloud/"}],"tags":[{"name":"oom","slug":"oom","permalink":"https://acodetailor.github.io/tags/oom/"},{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/tags/k8s/"},{"name":"knative","slug":"knative","permalink":"https://acodetailor.github.io/tags/knative/"},{"name":"terraform","slug":"terraform","permalink":"https://acodetailor.github.io/tags/terraform/"}]}