{"meta":{"title":"Sam","subtitle":"用我的双手，成就你的梦想。一库~","description":"杂记","author":"Sam","url":"https://acodetailor.github.io","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2021-03-30T07:24:39.640Z","updated":"2021-03-30T07:24:39.640Z","comments":false,"path":"/404.html","permalink":"https://acodetailor.github.io/404.html","excerpt":"","text":""},{"title":"书单","date":"2021-03-30T07:24:39.645Z","updated":"2021-03-30T07:24:39.644Z","comments":false,"path":"books/index.html","permalink":"https://acodetailor.github.io/books/index.html","excerpt":"","text":""},{"title":"关于","date":"2021-03-30T07:24:39.644Z","updated":"2021-03-30T07:24:39.643Z","comments":false,"path":"about/index.html","permalink":"https://acodetailor.github.io/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"分类","date":"2021-03-30T07:24:39.645Z","updated":"2021-03-30T07:24:39.645Z","comments":false,"path":"categories/index.html","permalink":"https://acodetailor.github.io/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2021-03-30T07:24:39.646Z","updated":"2021-03-30T07:24:39.646Z","comments":true,"path":"links/index.html","permalink":"https://acodetailor.github.io/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2021-03-30T07:24:39.647Z","updated":"2021-03-30T07:24:39.647Z","comments":false,"path":"repository/index.html","permalink":"https://acodetailor.github.io/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2021-03-30T07:24:39.648Z","updated":"2021-03-30T07:24:39.648Z","comments":false,"path":"tags/index.html","permalink":"https://acodetailor.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Serverless之Knative","slug":"serverless-istio","date":"2021-05-16T12:59:42.740Z","updated":"2021-05-16T13:48:47.655Z","comments":false,"path":"2021/05/16/serverless-istio/","link":"","permalink":"https://acodetailor.github.io/2021/05/16/serverless-istio/","excerpt":"","text":"一、什么是Knativeknative 是谷歌牵头的 serverless 架构方案，旨在提供一套简单易用的 serverless 开源方案，把 serverless 标准化和平台化。目前参与 knative 项目的公司主要有： Google、Pivotal、IBM、Red Hat和SAP。 这是 Google Cloud Platform 宣布 knative 时给出的介绍：Developed in close partnership with Pivotal, IBM, Red Hat, and SAP, Knative pushes Kubernetes-based computing forward by providing the building blocks you need to build and deploy modern, container-based serverless applications.与Pivotal，IBM，Red Hat和SAP密切合作开发，通过提供构建和部署基于容器的现代serverless应用程序所需的构建块，Knative推动基于Kubernetes的计算。 在knative的github页面，Knative 给出的官方介绍如下：Kubernetes-based platform to build, deploy, and manage modern serverless workloads.基于Kubernetes的平台，用于构建，部署和管理现代serverless工作负载。 二、Knative概述Knative扩展了Kubernetes，提供了一组中间件组件，这些组件对于构建可在任何地方运行的现代以源为中心和基于容器的应用程序至关重要：在本地，在云中，甚至在第三方数据中心。 Knative项目下的每个组件都尝试识别常见模式并编纂最佳实践，这些最佳实践被真实世界中基于Kubernetes的成功框架和应用程序共享。 Knative组件专注于解决许多平凡但困难的任务，例如： 部署容器 在Kubernetes上编排source-to-URL的工作流程 使用蓝绿部署路由和管理流量 根据需求自动收缩和调整工作负载大小 将运行服务绑定到事件生态系统 Knative的开发人员可以使用熟悉的习语，语言和框架来部署任何工作负载：函数，应用程序或容器。 组件 Build - 源到容器的构建编排 Eventing - 管理和交付事件 Serving - 请求驱动的计算，可以扩展到零 受众 开发人员Knative组件为开发人员提供Kubernetes原生API，用于将serverless风格的函数，应用程序和容器部署到自动伸缩运行时。 运维Knative组件旨在集成到更加优雅的产品中，云服务提供商或大型企业的内部团队可以随后运维。 任何企业或云提供商都可以将Knative组件应用到他们自己的系统中，并将这些收益传递给他们的客户。 Knative是一个多元化，开放和包容的社区。 三、Knative优势和已有的FaaS/serverless实现不同，knative在产品规划和设计理念上有带来新东西： 工作负载类型和标准化的 FaaS 不同，knative 期望能够运行所有的 workload : traditional application function container 针对常见应用用例提供更高级别抽象的聚焦API。 在几秒钟内即可提供可扩展，安全，无状态的服务。 松耦合特性可以按需使用组件 可插拔组件，可以自备日志和监控，网络和服务网格。Knative是可移植的：可运行于Kubernetes运行的任何地方，不用担心供应商锁定。knative 建立在 kubernetes 和 istio 之上 使用 kubernetes 提供的容器管理能力（deployment、replicaset、和 pods等），以及 istio 提供的网络管理功能（ingress、LB、dynamic route等）。 需要特别强调的是：knative 是 Kubernetes-based！ 或者说 Kubernetes-only，仅仅运行于k8s平台。","categories":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/tags/k8s/"}]},{"title":"Serverless之Knative流量转发","slug":"knative","date":"2021-04-30T01:40:25.368Z","updated":"2021-05-16T14:34:40.083Z","comments":false,"path":"2021/04/30/knative/","link":"","permalink":"https://acodetailor.github.io/2021/04/30/knative/","excerpt":"","text":"前边说过Knative是基于K8s部署的，使用k8s的容器管理功能以及istio的网络管理功能，下边一起看一下Knative的流量是怎么转发的吧。 一、Knative资源介绍Knative资源knative主要分为三个组件，build、server、event，分别处理CI/CD,服务伸缩，事件驱动。流量转发主要由server这部分处理。 knative service knative ingress knative serverlessService knative route knative configuration kantive revision istio资源 Gateway virtualService。 二、流量转发官方架构先回顾一下Knative官方的一个简单的原理示意图如下所示。用户创建一个Knative Service（ksvc）后，Knative会自动创建Route（route）、Configuration（cfg）资源，然后cfg会创建对应的Revision（rev）版本。rev实际上又会创建Deployment提供服务，流量最终会根据route的配置，导入到相应的rev中。 老版本（0.6）在集成使用Istio部署时，Route默认采用的是Istio Ingress Gateway实现，大概在Knative 0.6版本之前，我们可以发现，Route的流量转发本质上是由Istio virtualservice（vs）控制。副本数为0时，其中destination指向的是Activator组件。此时Activator会帮助转发冷启动时的请求。 当服务启动后，修改vs将destination指向对应服务实例上。 新版本我们创建一个简单的hello-go ksvc，并以此进行分析。ksvc如下所示： 12345678910111213apiVersion: serving.knative.dev/v1alpha1kind: Servicemetadata: name: hello-go namespace: faasspec: template: spec: containers: - image: harbor-yx-jd-dev.yx.netease.com/library/helloworld-go:v0.1 env: - name: TARGET value: &quot;Go Sample v1&quot; virtualservice的变化环境是一个标准的Istio部署，Serverless网关为Istio Ingress Gateway，所以创建完ksvc后，为了验证服务是否可以正常运行，需要发送http请求至网关。Gateway资源已经在部署Knative的时候创建，这里我们只需要关心vs。在服务副本数为0的时候，Knative控制器创建的vs关键配置如下： 12345678910111213141516171819202122232425262728spec: gateways: - knative-serving/cluster-local-gateway - knative-serving/knative-ingress-gateway hosts: - hello-go.faas - hello-go.faas.example.com - hello-go.faas.svc - hello-go.faas.svc.cluster.local - f81497077928a654cf9422088e7522d5.probe.invalid http: - match: - authority: regex: ^hello-go\\.faas\\.example\\.com(?::\\d&#123;1,5&#125;)?$ gateways: - knative-serving/knative-ingress-gateway - authority: regex: ^hello-go\\.faas(\\.svc(\\.cluster\\.local)?)?(?::\\d&#123;1,5&#125;)?$ gateways: - knative-serving/cluster-local-gateway retries: attempts: 3 perTryTimeout: 10m0s route: - destination: host: hello-go-fpmln.faas.svc.cluster.local port: number: 80 vs指定了已经创建好的gw，同时destination指向的是一个Service域名。这个Service就是Knative默认自动创建的hello-go服务的Service。可以发现vs的ownerReferences指向了一个Knative的CRD ingress.networking.internal.knative.dev： 1234567ownerReferences:- apiVersion: networking.internal.knative.dev/v1alpha1 blockOwnerDeletion: true controller: true kind: Ingress name: hello-go uid: 4a27a69e-5b9c-11ea-ae53-fa163ec7c05f 根据名字可以看到这是一个Knative内部使用的CRD，该CRD的内容其实和vs比较类似，同时ingress.networking.internal.knative.dev的ownerReferences指向了我们熟悉的route，总结下来就是：route -&gt; kingress(ingress.networking.internal.knative.dev) -&gt; vs在网关这一层涉及到的CRD资源就是如上这些。这里kingress的意义在于增加一层抽象，如果我们使用的是其他网关，则会将kingress转换成相应的网关资源配置。最新的版本中，负责kingress到Istio vs的控制器部分代码已经独立出一个项目，可见如今的Knative对Istio已经不是强依赖。现在，我们已经了解到Serverless网关是由Knative控制器最终生成的vs生效到Istio Ingress Gateway上，为了验证我们刚才部署的服务是否可以正常的运行，简单的用curl命令试验一下。 和所有的网关或者负载均衡器一样，对于7层http访问，我们需要在Header里加域名Host，用于流量转发到具体的服务。在上面的vs中已经可以看到对外域名和内部Service域名均已经配置。所以，只需要： 1curl -v -H&#x27;Host:hello-go.faas.example.com&#x27; &lt;IngressIP&gt;:&lt;Port&gt; 其中，IngressIP即网关实例对外暴露的IP。 对于冷启动来说，目前的Knative需要等十几秒，即会收到请求。根据之前老版本的经验，这个时候vs会被更新，destination指向hello-go的Service。 不过，现在我们实际发现，vs没有任何变化，仍然指向了服务的Service。对比老版本中服务副本数为0时，其实vs的destination指向的是Activator组件的。但现在，不管服务副本数如何变化，vs一直不变。 蹊跷只能从destination的Service域名入手。 revision service探索创建ksvc后，Knative会帮我们自动创建Service如下所示。 123456$ kubectl -n faas get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) hello-go ExternalName &lt;none&gt; cluster-local-gateway.istio-system.svc.cluster.local &lt;none&gt; hello-go-fpmln ClusterIP 10.178.4.126 &lt;none&gt; 80/TCP hello-go-fpmln-m9mmg ClusterIP 10.178.5.65 &lt;none&gt; 80/TCP,8022/TCP hello-go-fpmln-metrics ClusterIP 10.178.4.237 &lt;none&gt; 9090/TCP,9091/TCP hello-go Service是一个ExternalName Service，作用是将hello-go的Service域名增加一个dns CNAME别名记录，指向网关的Service域名。 根据Service的annotation我们可以发现，Knative对hello-go-fpmln、hello-go-fpmln-m9mmg 、hello-go-fpmln-metrics这三个Service的定位分别为public Service、private Service和metric Service（最新版本已经将private和metrics Service合并）。 private Service和metric Service其实不难理解。问题的关键就在这里的public Service，仔细研究hello-go-fpmln Service，我们可以发现这是一个没有labelSelector的Service，它的Endpoint不是kubernetes自动创建的，需要额外生成。 在服务副本数为0时，查看一下Service对应的Endpoint，如下所示： 12345$ kubectl -n faas get epNAME ENDPOINTS AGEhello-go-fpmln 172.31.16.81:8012 hello-go-fpmln-m9mmg 172.31.16.121:8012,172.31.16.121:8022 hello-go-fpmln-metrics 172.31.16.121:9090,172.31.16.121:9091 其中，public Service的Endpoint IP是Knative Activator的Pod IP，实际发现Activator的副本数越多这里也会相应的增加。并且由上面的分析可以看到，vs的destination指向的就是public Service。 输入几次curl命令模拟一下http请求，虽然副本数从0开始增加到1了，但是这里的Endpoint却没有变化，仍然为Activator Pod IP。 接着使用hey来压测一下： 12./hey_linux_amd64 -n 1000000 -c 300 -m GET -host helloworld-go.faas.example.com http://&lt;IngressIP&gt;:80 发现Endpoint变化了，通过对比服务的Pod IP，已经变成了新启动的服务Pod IP，不再是Activator Pod的IP。 12345$ kubectl -n faas get epNAME ENDPOINTS helloworld-go-mpk25 172.31.16.121:8012hello-go-fpmln-m9mmg 172.31.16.121:8012,172.31.16.121:8022 hello-go-fpmln-metrics 172.31.16.121:9090,172.31.16.121:9091 原来，现在新版本的冷启动流量转发机制已经不再是通过修改vs来改变网关的流量转发配置了，而是直接更新服务的public Service后端Endpoint，从而实现将流量从Activator转发到实际的服务Pod上。 通过将流量的转发功能内聚到Service/Endpoint层，一方面减小了网关的配置更新压力，一方面Knative可以在对接各种不同的网关时的实现时更加解耦，网关层不再需要关心冷启动时的流量转发机制。 流量路径再深入从上述的三个Service入手研究，它们的ownerReference是serverlessservice.networking.internal.knative.dev(sks)，而sks的ownerReference是podautoscaler.autoscaling.internal.knative.dev(kpa)。 在压测过程中同样发现，sks会在冷启动过后，会从Proxy模式变为Serve模式： 123456$ kubectl -n faas get sksNAME MODE SERVICENAME PRIVATESERVICENAME READY REASONhello-go-fpmln Proxy hello-go-fpmln hello-go-fpmln-m9mmg True$ kubectl -n faas get sksNAME MODE SERVICENAME PRIVATESERVICENAME READY REASONhello-go-fpmln Serve hello-go-fpmln hello-go-fpmln-m9mmg True 这也意味着，当流量从Activator导入的时候，sks为Proxy模式，服务真正启动起来后会变成Serve模式，网关流量直接流向服务Pod。 从名称上也可以看到，sks和kpa均为Knative内部CRD，实际上也是由于Knative设计上可以支持自定义的扩缩容方式和支持Kubernetes HPA有关，实现更高一层的抽象。","categories":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/categories/k8s/"}],"tags":[{"name":"knative","slug":"knative","permalink":"https://acodetailor.github.io/tags/knative/"}]},{"title":"Serverless之介绍","slug":"serverless","date":"2021-04-28T06:35:10.932Z","updated":"2021-05-16T13:55:54.901Z","comments":false,"path":"2021/04/28/serverless/","link":"","permalink":"https://acodetailor.github.io/2021/04/28/serverless/","excerpt":"","text":"一、Serverless是什么Serverless计算是指构建和运行不需要服务器管理的应用程序的概念。它描述了一种更细粒度的部署模型，应用程序捆绑一个或多个function，上载到平台，然后执行，缩放和计费，以响应当前所需的确切需求。Serverless计算并不意味着我们不再使用服务器来托管和运行代码；也不意味着不再需要运维工程师。相反，它指的是serverless计算的消费者（开发人员）不再需要花费时间和资源来进行服务器配置，维护，更新，扩展和容量规划。相反，所有这些任务和功能都由serverless平台处理，并完全从开发人员和IT/运维团队中抽象出来。因此，开发人员专注于编写应用程序的业务逻辑。运维工程师能够将重点更多的放到关键业务任务上。 二、Serverless的发展在软件研发领域，绕不开的 2 个环节是软件的部署和运维。如果我们要上线一个业务。在 On-Premise 阶段，需要购买物理服务器，可能还需要自建机房、安装制冷设备、招聘运维人员，然后再在上面搭建一系列的基础设施，比如：虚拟化、操作系统、容器、Runtime，Runtime 可以理解为像 Python、golang、Node.js 这类软件。接下来我们要去安装软件类的开发框架。最后，我们才会去编写我们真正需要的业务函数。在IaaS阶段，云厂商维护了硬件和虚拟化这 2 个基础设施，到了 PaaS 层云厂商又维护了 OS、容器和 Runtime。在FaaS阶段，用户只需要关注 Function，也就是只需要关注自己的业务逻辑。可以看到随着阶段的演进用户需要关注的点越来越少，越来越聚焦于自己的业务逻辑。所以在 On-Premise 阶段我们开发一个业务可能需要 8 个人，在 FaaS 阶段，我们只需要 2 个业务，节省的人力可以投入到业务研发这块儿，提高产品的迭代速度，进而提高产品竞争力。过去十多年，云计算其实是一个「去基础架构」的过程。这个过程可以让用户聚焦于自己真正需要的业务开发上，而不是底层的计算资源上。Serverless 符合云计算发展的方向，是云的终极形态，这种特有的模式使 Serverless 存在潜在的巨大价值。 三、Serverless的角色：Developer/开发人员：为serverless平台编写代码并从中获益，serverless平台提供了这样的视角：没有服务器，而代码始终在运行。Provider/平台管理者：为外部或内部客户部署serverless平台。运行serverless平台仍然需要服务器。平台管理者需要管理服务器（或虚拟机和容器）。即使在空闲时，平台管理者也会有一些运行平台的成本。通常一个团队充当平台管理者，其他的团队充当开发人员。 四、Serverless提供什么：Functions-as-a-Service (FaaS) ，通常提供事件驱动计算。开发人员使用由事件或HTTP请求触发的function来运行和管理应用程序代码。 开发人员将代码的小型单元部署到FaaS，这些代码根据需要作为离散动作执行，无需管理服务器或任何其他底层基础设施即可进行扩展。Backend-as-a-Service (BaaS) ，它是基于API的第三方服务，可替代应用程序中的核心功能子集。因为这些API是作为可以自动扩展和透明操作的服务而提供的，所以对于开发人员表现为是serverless。 五、Serverless为开发人员带来什么好处： 零服务器运维：serverless通过消除维护服务器资源所涉及的开销，显着改变了运行软件应用程序的成本模型。 无需配置，更新和管理服务器基础设施：管理服务器，虚拟机和容器对于公司而言是一项重大费用，其中包括人员，工具，培训和时间。Serverless大大减少了这种费用。 灵活的可扩展性：serverless的FaaS或BaaS产品可以即时而精确地扩展，以处理每个单独的传入请求。对于开发人员来说，serverless平台没有“预先计划容量”的概念，也不需要配置“自动缩放”的触发器或规则。缩放可以在没有开发人员干预的情况下自动进行。完成请求处理后，serverless FaaS会自动收缩计算资源，因此不会有空闲容量。 闲置时无计算成本：从消费者的角度来看，serverless产品的最大好处之一是空闲容量不会产生任何成本。例如，serverless计算服务不对空闲虚拟机或容器收费; 换句话说，当代码没有运行或者没有进行有意义的工作时，不收费。 对于数据库，数据库引擎容量空闲等待请求时无需收费。当然，这不包括其他成本，例如有状态存储成本或添加的功能/功能/特性集。 六、Serverless用例虽然serverless计算广泛使用，但它仍然相对较新。通常，当工作负载为以下情况时，应将serverless方法视为首选： 异步，并发，易于并行化为独立的工作单元 不经常或有零星的需求，在扩展要求方面存在巨大的，不可预测的差异 无状态，短暂的，对瞬间冷启动时间没有重大需求 在业务需求变更方面具有高度动态性，需要加快开发人员的速度 多媒体处理一个常见的用例，也是最早具体化的用例之一，是响应新文件上传执行一些转换过程的函数。 例如，如果将图像上载到诸如Amazon S3的对象存储服务，则该事件触发函数，用于创建图像的缩略图版本并将其存储回另一个对象存储桶或Database-as-a-Service。 这是一个相当原子化，可并行化的计算任务示例，该计算任务不经常运行并根据需求进行伸缩。 例子包括：通过将每个视频帧通过图像识别服务来自动分类电影，以提取演员，情感和对象; 或处理灾区的无人机镜头以估计损坏的程度。 数据库更改或更改数据捕获（CDC）在此场景中，当从数据库插入，修改或删除数据时调用function。在这种情况下，它的功能类似于传统的SQL触发器，几乎就像是与主同步流并行的副作用或动作。其结果是执行一个异步逻辑，可以修改同一个数据库中的某些内容（例如记录到审计表），或者依次调用外部服务（例如发送电子邮件）或更新其他数据库，例如 DB CDC（更改数据捕获）用例的情况。 由于业务需要和处理变更的服务分布的原因，这些用例的频率以及对原子性和一致性的需要可能不同。 例子包括： 审核对数据库的更改，或确保它们满足特定质量或分析标准以进行可接受的更改。 在输入数据时或之后不久自动将数据翻译为其他语言。 IoT/物联网传感器输入消息随着连接到网络的自主设备的爆炸式增加，额外的流量体积庞大，并且使用比HTTP更轻量级的协议。 高效的云服务必须能够快速响应消息并扩展以响应其扩散或突然涌入的消息。Serverless功能可以有效地管理和过滤来自IoT设备的MQTT消息。 它们既可以弹性扩展，也可以屏蔽负载下游的其他服务。 例子包括： 在物联网设备（如AWS Greengrass）上使用serverless来收集本地传感器数据，对其进行规范化，与触发器进行比较，并将事件推送到聚合单元/云。 大规模流处理#另一种非事务性，非请求/响应类型的工作负载是在可能无限的消息流中处理数据。 函数可以连接到消息源，而消息必须从事件流中读取和处理。 鉴于高性能，高弹性和计算密集型处理工作负载，这对于serverless而言非常重要。 在许多情况下，流处理需要将数据与一组上下文对象（在NoSQL或in-mem DB中）进行比较，或者将数据从流聚合并存储到对象或数据库系统中。 例子包括： Mike Roberts有一个很好的 Java/AWS Kinesis 示例 ，可以有效地处理数十亿条消息。 SnapChat 在Google AppEngine上使用serverless 处理邮件。 聊天机器人#与人类交互不一定需要毫秒级别的响应时间，并且在许多方面，稍微延迟让回复人类的机器人对话感觉更自然。因此，等待从冷启动加载function的初始等待时间可能是可接受的。当添加到Facebook，WhatsApp或Slack等流行的社交网络时，机器人可能还需要具有极高的可扩展性，因此在PaaS或IaaS模型中预先设置一个永远在线的守护程序，以预测突然或高峰需求，可能不会有作为serverless方法的高效或成本效益。 例子包括： 支持和销售机器人插入到大型社交媒体服务，如Facebook或其他高流量网站。 消息应用程序Wuu使用Google Cloud Functions使用户能够创建和共享在数小时或数秒内消失的内容。 另请参阅下面的HTTP REST API和Web应用程序。 批处理作业或计划任务#每天只需几分钟就能以异步方式进行强大的并行计算，IO或网络访问的作业非常适合serverless。作业可以在以弹性方式运行时有效地消费他们所需的资源，并且在不被使用的当天剩余时间内不会产生资源成本。 例子包括： 计划任务可以是每晚运行的备份作业。并行发送许多电子邮件的作业会扩展function实例。 HTTP REST API和Web应用程序#传统的请求/响应工作负载仍然非常适合serverless，无论工作负载是静态网站还是使用JavaScript或Python等编程语言按需生成响应。即使它们可能会为第一个用户带来启动成本，但在其他计算模型中存在这种延迟的先例，例如将JavaServer Page初始编译为servlet，或者启动新的JVM来处理额外的负载。好处是单个REST调用（例如，微服务中的GET，POST，UPDATE和DELETE 4端点中的每一个）可以独立扩展并单独计费，即使它们共享公共数据后端。 例子包括： 移植到serverless架构的澳大利亚人口普查显示了开发速度，成本改进和自动扩展。“如何通过无服务器将我的AWS账单削减90％。”AutoDesk示例：“成本只占传统云方法的一小部分（约1％）。”在线编码/教育（考试，测试等）在事件驱动的环境中运行训练代码，并基于与该训练的结果和预期结果的比较向用户提供反馈。Serverless平台根据需要运行应答检查并根据需要进行扩展，仅在代码运行的时间内需要付费。 移动后端#使用serverless进行移动后端任务也很有吸引力。它允许开发人员在BaaS API之上构建REST API后端工作负载，因此他们可以花时间优化移动应用程序，而不是扩展后端。 例子包括：优化视频游戏的图形，而不是在游戏成为病毒式打击时投资服务器; 或者对于需要快速迭代以发现产品/市场适合性，或者上市时间至关重要的消费者业务应用程序。另一个示例是批量通知用户或程序其他异步任务以获得离线优先体验。 例子包括： 需要少量服务器端逻辑的移动应用程序; 开发人员可以将精力集中在原生代码开发上。使用已配置的安全策略（例如Firebase身份验证/规则或Amazon Cognito）通过事件触发的serverless计算使用直接从移动设备访问BaaS的移动应用程序。“Throwaway”或短期使用的移动应用程序，例如大型会议的调度应用程序，在会议前后的周末几乎没有需求，但需要极大的扩展和收缩; 在周一和周二早上的活动过程中根据时间表查看要求，然后在午夜时分回到主题演讲。 业务逻辑#当与管理和协调function一起部署时，在业务流程中执行一系列步骤的微服务工作负载的编排是serverless计算的另一个好用例。执行特定业务逻辑的function（例如订单请求和批准，股票交易处理等）可以与有状态管理器一起安排和协调。来自客户端门户的事件请求可以由这样的协调function提供服务，并传递给适当的serverless function。 例子包括： 交易台，处理股票市场交易并处理客户的交易订单和确认。协调器使用状态图管理交易。初始状态接受来自客户端门户的交易请求，并将请求传递给微服务function以解析请求并验证客户端。随后的状态根据买入或卖出交易指导工作流，验证基金余额，股票代码等，并向客户发送确认。在从客户端接收到确认请求事件时，后续状态调用管理交易执行的function，更新账户，并通知客户完成交易。 持续集成管道#传统的CI管道包括一个构建从属主机池，它们处于空闲等待以便分派作业。Serverless是一种很好的模式，可以消除对预配置主机的需求并降低成本。构建作业由新代码提交或PR合并触发。 调用function来运行构建和测试用例，仅在所需的时间内执行，并且在未使用时不会产生成本。这降低了成本，并可通过自动扩展来减少瓶颈以满足需求。","categories":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/tags/k8s/"}]},{"title":"java的OOM问题","slug":"java_oom","date":"2021-04-24T12:11:36.499Z","updated":"2021-04-28T06:34:43.262Z","comments":false,"path":"2021/04/24/java_oom/","link":"","permalink":"https://acodetailor.github.io/2021/04/24/java_oom/","excerpt":"","text":"一、一些概念JVM中的堆被划分为两个不同区域：新生代Young、老年代Old。新生代又划分为Eden(伊甸，标志新生)， Survivor0(s0)， Survivor(s1)。JVM中堆的GC分为：Minor GC 和 Full GC(又称为Major GC) 年轻代年轻代用于存放新创建的对象，存储大小默认为堆大小的1/15，特点是对象更替速度快，即短时间内产生大量的“死亡对象”-Xmn 可以设置年轻代为固定大小-XX:NewRatio 可以设置年轻代与老年代的大小比例年轻代使用复制-清除算法和并行收集器进行垃圾回收，对年轻代的垃圾回收称为初级回收（minor GC)minor GC 将年轻代分为三个区域，一个Eden, 两个大小相同的Survivor。应用程序只能同时使用一个Eden和一个活动Survivor, 另外一个Survivor为非活动Survivor（两个Survivor在活动与非活动间交替存在，即同一时刻只存在一个活动Survivor和一个非活动Survivor）。当发生 minor GC时：JVM执行下述操作 将程序挂起 将Eden和活动Survivor中的存活对象（存活对象指的是仍被引用的对象）复制到另一个非活动的Survivor中（记录对象被复制到另一个Survivor的次数，在此称为年龄数，每次复制+1） 清除Eden和活动Survivor中对象 将非活动Survivor标记为活动，将原来的活动Survivor标记为非活动上述即为一轮 minor GC 结束如此往复多次 minor GC后，将多次被复制的对象（高龄对象）移动到老年代中默认被移动到老年代的年龄为15，可以通过参数 -XX:MaxTenuring Threshold 来设置。当然，对于一些占用较大内存的对象，会被直接送入老年代 老年代老年代存储的是那些不会轻易“死掉”的对象，毕竟都在年轻代中熬出了头。在老年代发生的GC成为 Full GC 。Full GC 不会像 Minor GC那么频繁Full GC 采用 标记-清除算法 收集垃圾的时候会产生许多内存碎片（不连续的存储空间），因此此后若有较大的对象要进入老年代而无法找到适合的存储空间，就会提前触发一次GC收集，对内存空间进行整理 永久代永久代（Perm Gen）是JDK7中的特性，JDK8后被取消。永久区是JVM方法区的实现方式之一，JDK8起，被元空间（与堆不相连的本地空间）取而代之永久代存放的是应用元数据（应用中使用的类和方法），永久代中的对象在 Full GC 的时候进行垃圾回收 简单来讲，jvm的内存回收过程是这样的：对象在Eden Space创建，当Eden Space满了的时候，gc就把所有在Eden Space中的对象扫描一次，把所有有效的对象复制到第一个Survivor Space，同时把无效的对象所占用的空间释放。当Eden Space再次变满了的时候，就启动移动程序把Eden Space中有效的对象复制到第二个Survivor Space，同时，也将第一个Survivor Space中的有效对象复制到第二个Survivor Space。如果填充到第二个Survivor Space中的有效对象被第一个Survivor Space或Eden Space中的对象引用，那么这些对象就是长期存在的，此时这些对象将被复制到Permanent Generation。 若垃圾收集器依据这种小幅度的调整收集不能腾出足够的空间，就会运行Full GC，此时jvm gc停止所有在堆中运行的线程并执行清除动作。 二、优化配置-Xms 是指程序启动时初始内存大小（此值可以设置成与-Xmx相同，以避免每次GC完成后 JVM 内存重新分配）。默认为物理内存的1/64，最小为1M；可以指定单位，比如k、m，若不指定，则默认为字节。-Xmx 指程序运行时最大可用内存大小，程序运行中内存大于这个值会 OutOfMemory。默认为物理内存的1/4或者1G，最小为2M；单位与-Xms一致-Xmn 年轻代大小（整个JVM内存大小 = 年轻代 + 年老代 + 永久代）。-XX:NewRatio 年轻代与年老代的大小比例，-XX:NewRatio=4 设置为4，则年轻代与年老代所占比值为1：4。-XX:SurvivorRatio 年轻代中Eden区与Survivor区的大小比值，-XX:SurvivorRatio=4，设置为4，则两个Survivor区与一个Eden区的比值为 2:4-XX:MaxPermSize 设置永久代大小。-XX:MaxTenuringThreshold 设置垃圾最大年龄。如果设置为0的话，则年轻代对象不经过Survivor区，直接进入年老代。对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象再年轻代的存活时间，增加在年轻代即被回收的概论。-Xss 设置每个线程的堆栈大小。默认为512k 三、容器化场景配置使用容器内存大小，避免Cgroup的oomJDK 8u131 在 JDK 9 中有一个特性，可以在 Docker 容器运行时能够检测到多少内存的能力。在容器内部运行 JVM，它在大多数情况下将如何默认最大堆为主机内存的1/4，而非容器内存的1/4。如果对 Java 容器中的jvm虚拟机不做任何限制，当我们同时运行几个 java 容器时，很容易导致服务器的内存耗尽、负载飙升而宕机；而如果我们对容器直接进行限制，就会导致内核在某个时候杀死jvm 容器而导致频繁重启。 12345$ docker run -m 100MB openjdk:8u121 java -XshowSettings:vm -versionVM settings: Max. Heap Size (Estimated): 444.50M Ergonomics Machine Class: server Using VM: OpenJDK 64-Bit Server VM 下面我们尝试 JDK 8u131 中的实验性参数 -XX:+UseCGroupMemoryLimitForHeap 12345678$ docker run -m 100MB openjdk:8u131 java \\ -XX:+UnlockExperimentalVMOptions \\ -XX:+UseCGroupMemoryLimitForHeap \\ -XshowSettings:vm -versionVM settings: Max. Heap Size (Estimated): 44.50M Ergonomics Machine Class: server Using VM: OpenJDK 64-Bit Server VM JVM能够检测容器只有100MB，并将最大堆设置为44M。 下面尝试一个更大的容器 12345678$ docker run -m 1GB openjdk:8u131 java \\ -XX:+UnlockExperimentalVMOptions \\ -XX:+UseCGroupMemoryLimitForHeap \\ -XshowSettings:vm -versionVM settings: Max. Heap Size (Estimated): 228.00M Ergonomics Machine Class: server Using VM: OpenJDK 64-Bit Server VM 嗯，现在容器有1GB，但JVM仅使用228M作为最大堆。 除了JVM正在容器中运行以外，我们是否还可以优化它呢？ 12345678$ docker run -m 1GB openjdk:8u131 java \\ -XX:+UnlockExperimentalVMOptions \\ -XX:+UseCGroupMemoryLimitForHeap \\ -XX:MaxRAMFraction=1 -XshowSettings:vm -versionVM settings: Max. Heap Size (Estimated): 910.50M Ergonomics Machine Class: server Using VM: OpenJDK 64-Bit Server VM 使用-XX:MaxRAMFraction 我们告诉JVM使用可用内存/ MaxRAMFraction作为最大堆。使用-XX:MaxRAMFraction=1我们几乎所有可用的内存作为最大堆。","categories":[{"name":"运维","slug":"运维","permalink":"https://acodetailor.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"oom","slug":"oom","permalink":"https://acodetailor.github.io/tags/oom/"}]},{"title":"Minikube搭建istio和Knative","slug":"install knative","date":"2021-04-19T09:26:51.731Z","updated":"2021-04-28T06:32:58.096Z","comments":false,"path":"2021/04/19/install knative/","link":"","permalink":"https://acodetailor.github.io/2021/04/19/install%20knative/","excerpt":"","text":"需要搭建一套knative的测试环境，但是没有测试集群可以用了，就用自己的电脑搭建了一个测试环境。搭建过程记录一下 环境信息os:MacDriver:VirtualBox 搭建minikube一条命令即可,可以指定cpu和内存–cpus int –memory int 1minikube start --vm-driver=virtualbox --image-repository=&#x27;registry.cn-hangzhou.aliyuncs.com/google_containers&#x27; minikube常用命令 123456789101112131415161718191. minikube start 启动minikube2. minikube dashboard 打开dashboard3. minikube version 查看minikube版本4. minikube status 查看集群状态5. minikube ip 显示虚拟机ip地址6. minikube stop 停止虚拟机7. minikube ssh ssh到虚拟机中8. minikube delete 删除虚拟机9. minikube logs 查看虚拟机日志10. minikube update-check 检查更新11. minikube node list[add|start|stop|delete] 对节点进行操作12. minikube mount 将指定的目录挂载到minikube13. minikube docker-env 配置环境以使用minikube的docker守护进程14. minikube podman-env 配置环境以使用minikube的Podman服务15. minikube cache 添加，删除，或推送一个本地映像到minikube16. minikube addons 启用或禁用一个minikube插件17. minikube config 修改持久化配置值18. minikube profile 获取或者列出当前的配置文件（集群）19. minikube update-context 在IP或者端口改变的情况下更新kubeconfig 安装成功后提示如下： 12345678910😄 Darwin 10.14.6 上的 minikube v1.8.2✨ 根据现有的配置文件使用 virtualbox 驱动程序✅ 正在使用镜像存储库 registry.cn-hangzhou.aliyuncs.com/google_containers💾 Downloading preloaded images tarball for k8s v1.17.3 ...⌛ 重新配置现有主机🏃 Using the running virtualbox &quot;minikube&quot; VM ...🐳 正在 Docker 19.03.6 中准备 Kubernetes v1.17.3…🚀 正在启动 Kubernetes ... 🌟 Enabling addons: default-storageclass, storage-provisioner🏄 完成！kubectl 已经配置至 &quot;minikube&quot; 搭建istio下载istio的包，然后根据提示配置环境变量，由于某些已知原因，可以配置host 1199.232.28.133 raw.githubusercontent.com 1curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.9.1 TARGET_ARCH=x86_64 sh - 安装命令：根据需要选择合适的profile 12istioctl manifest apply --set profile=demo //使用这条命令安装失败了。istioctl install --set profile=demo -y 追加部署addons 12cd istio-1.9.1kc --context=minikube apply -f samples/addons -n istio-system 搭建knativeknative搭建比较简单，可以手动安装，再麻烦一点，可以自己下载yaml手动执行，serving和eventing一共四个yaml 1234serving-crdsserving-coreeventing-crdseventing-core 1kubectl apply --filename &quot;https://github.com/knative/serving/releases/download/v0.17.0/serving-crds.yaml&quot; 比较麻烦的是镜像，国内无法直接下载到，可以手动下载后，使用minikube cache 添加到minikube的节点上。 结果","categories":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/tags/k8s/"}]},{"title":"记一次OOM的问题排查","slug":"ops-oom","date":"2021-04-12T13:03:40.025Z","updated":"2021-04-28T06:34:43.266Z","comments":false,"path":"2021/04/12/ops-oom/","link":"","permalink":"https://acodetailor.github.io/2021/04/12/ops-oom/","excerpt":"","text":"一、问题现象 三天前，业务的pod重启了，业务想知道具体原因。 二、排查记录 首先排查kubelet日志，蛋疼的是期间业务手动重启了几次，导致不知道pod重启时所在的节点。一个个节点排查kubelet的日志（不知道后续有没有好的方法），找到节点发现日志如下： kubelet执行PLEG时发现容器挂掉了，直接给拉起了，就是业务前台发现的重启事件。 再排查业务日志，发现只有Killed,八成是OOM了。日志截图如下： 下一步排查内核日志，kerenal。发现确实是OOM了。截图如下: 然后问题来了，我们的监控发现并没有达到内存的上限，但是cgroup确实达到了上限。真是让人头大啊。 三、几个命令12zgrep &quot;&quot; *.gzjournalctl --since &#x27;2021-04-09 22:00&#x27; 四、引申问题 1、cgroup的机制 2、java的内存机制","categories":[{"name":"运维","slug":"运维","permalink":"https://acodetailor.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"oom","slug":"oom","permalink":"https://acodetailor.github.io/tags/oom/"}]},{"title":"","slug":"TCP","date":"2021-04-09T06:37:26.865Z","updated":"2021-04-09T06:37:26.865Z","comments":true,"path":"2021/04/09/TCP/","link":"","permalink":"https://acodetailor.github.io/2021/04/09/TCP/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"","slug":"Iptabels And Ipvs","date":"2021-04-09T06:37:13.851Z","updated":"2021-04-09T06:37:13.851Z","comments":true,"path":"2021/04/09/Iptabels And Ipvs/","link":"","permalink":"https://acodetailor.github.io/2021/04/09/Iptabels%20And%20Ipvs/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"terraform cloud CLI","slug":"terraform_cloud","date":"2021-04-07T02:02:39.990Z","updated":"2021-04-28T06:34:43.256Z","comments":false,"path":"2021/04/07/terraform_cloud/","link":"","permalink":"https://acodetailor.github.io/2021/04/07/terraform_cloud/","excerpt":"","text":"一、简介 terraform自身提供可一个机制（类似CI/CD的流水线），可以配置自己的git库，根据git库文件内容的变化，执行plan、apply，也可以手动执行。以及配置变量文件、环境变量、state文件统一存储等等功能。 二、对比1、使用terraform二进制。 需要下载provider(terraform init) 编写tf文件，backend存储。也需要统一的配置管理。 每次手动触发。 2、使用Cloud CLI。 不需要准备provider 统一的配置管理 git push后即触发 terraform 三、栗子1、创建组织 terraform网站创建即可。 https://app.terraform.io2、配置git库 可以使用个人的git库，配置Oauth权限。根据提示 在github-&gt;setting-&gt;developer settings-&gt; oauth apps申请配置即可。 3、选择工程 git新建一个工程，然后再terraform处选择该工程。 4、测试 执行Run即可，每次修改都会触发 plan,然后需要手动执行apply。 5、结果","categories":[{"name":"terraform","slug":"terraform","permalink":"https://acodetailor.github.io/categories/terraform/"}],"tags":[{"name":"terraform","slug":"terraform","permalink":"https://acodetailor.github.io/tags/terraform/"}]},{"title":"Kubernetes CRD -- kubebuilder搭建","slug":"k8s_crd","date":"2021-04-02T02:17:18.350Z","updated":"2021-04-28T06:34:43.250Z","comments":false,"path":"2021/04/02/k8s_crd/","link":"","permalink":"https://acodetailor.github.io/2021/04/02/k8s_crd/","excerpt":"","text":"概念官方解释：CustomResourceDefinition API 资源允许你定义定制资源。 定义 CRD 对象的操作会使用你所设定的名字和模式定义（Schema）创建一个新的定制资源， Kubernetes API 负责为你的定制资源提供存储和访问服务。 CRD 对象的名称必须是合法的 DNS 子域名。 DNS子域名规则如下： 不能超过253个字符 只能包含小写字母、数字，以及’-‘ 和 ‘.’ 须以字母数字开头 须以字母数字结尾 简单来说，你可以定义像k8s原生资源如deployment、service一样定义自己的资源，而k8s会为你提供存储（ETCD）,访问（kube-apiserver）。 脚手架kubebuilder 和 operator sdk, 个人只使用过kubebuilder。 安装kubebuilder依赖1、docker2、go 建议版本在1.12以上，支持 go mod3、kustomize 代理配置配置一下 终端代理，因为初始化时会拉去go的依赖包。go 1.12之下开启 go module 12export GOPROXY=https://goproxy.ioexport GO111MODULE=on 安装一、安装kubebuilder执行如下命令。（如果curl下载失败，大概率是网络原因，可以手动下载，解压到指定目录）。 12345os= $(go env GOOS)arch=$(go env GOARCH)curl -L https://go.kubebuilder.io/dl/2.3.1/$&#123;os&#125;/$&#123;arch&#125; | tar -xz -C /tmp/sudo mv /tmp/kubebuilder_2.3.1_$&#123;os&#125;_$&#123;arch&#125; /usr/local/kubebuilderexport PATH=$PATH:/usr/local/kubebuilder/bin 二、安装kustomize 1brew install kustomize 三、安装完成后查看版本信息 12345$ kubebuilder versionVersion: version.Version&#123;KubeBuilderVersion:&quot;2.3.1&quot;, KubernetesVendor:&quot;1.16.4&quot;, GitCommit:&quot;8b53abeb4280186e494b726edf8f54ca7aa64a49&quot;, BuildDate:&quot;2020-03-26T16:42:00Z&quot;, GoOs:&quot;unknown&quot;, GoArch:&quot;unknown&quot;&#125;# sam @ MacBook-Pro-2 in ~/tf/alitest [10:45:37]$ kustomize version&#123;Version:kustomize/v3.8.2 GitCommit:e2973f6ecc9be6187cfd5ecf5e180f842249b3c6 BuildDate:2020-09-02T07:01:55+01:00 GoOs:darwin GoArch:amd64&#125; 四、初始化工程 12kubebuilder init --domain my.crd.com //初始化工程kubebuilder create api --group custom --version v1 --kind Unit //生成脚手架代码 group: 比如资源文件的apps/v1, apps即为分组，还有其他extensions、cores等。 version: 顾名思义、v1即为版本。 kind: API “顶级”资源对象的类型，每个资源对象都需要 Kind 来区分它自身代表的资源类型。比如 pod,deployment. resource: 通过 HTTP 协议以 JSON 格式发送或者读取的资源展现形式，可以以单个资源对象展现。 GVK(group、version、kind):同 Kind 不止可以出现在同一分组的不同版本中，如 apps/v1beta1 与 apps/v1，它还可能出现在不同的分组中，例如 Deployment 开始以 alpha 的特性出现在 extensions 分组，GA 之后被推进到 apps 组，所以为了严格区分不同的 Kind，需要组合 API Group、API Version 与 Kind 成为 GVK。 GVR(group、version、resource):GVR 常用于组合成 RESTful API 请求路径。例如，针对应用程序 v1 部署的 RESTful API 请求如下所示： 1GET /apis/apps/v1/namespaces/&#123;namespace&#125;/deployments/&#123;name&#125;","categories":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/tags/k8s/"}]},{"title":"Terraform介绍","slug":"terraform","date":"2021-03-30T02:38:39.714Z","updated":"2021-04-28T06:34:43.270Z","comments":false,"path":"2021/03/30/terraform/","link":"","permalink":"https://acodetailor.github.io/2021/03/30/terraform/","excerpt":"","text":"简介：Terraform是IT 基础架构自动化编排工具，它的口号是 “Write,Plan, and create Infrastructure as Code”, 基础架构即代码。 怎么理解这句话，我们先假设在没有Terraform的年代我们是怎么操作云服务。 方式一：直接登入到云平台的管控页面，人工点击按钮、键盘敲入输入参数的方式来操作，这种方式对于单个或几个云服务器还可以维护的过来，但是当云服务规模达到几十几百甚至上千以后，明显这种方式对于人力来说变得不再现实，而且容易误操作。 方式二：云平台提供了各种SDK，将对云服务的操作拆解成一个个的API供使用厂商通过代码来调用。这种方式明显好于方式一，使大批量操作变得可能，而且代码测试通过后可以避免人为误操作。但是随之带来的问题是厂商们需要专业的开发人员（Java、Python、Php、Ruby等），而且对复杂云平台的操作需要写大量的代码。 方式三：云平台提供了命令行操作云服务的工具，例如AWS CLI，这样租户厂商不再需要软件开发人员就可以实现对平台的命令操作。命令就像Sql一样，使用增删改查等操作元素来管理云。 方式四：Terraform主角登场，如果说方式三中CLI是命令式操作，需要明确的告知云服务本次操作是查询、新增、修改、还是删除，那么Terraform就是目的式操作，在本地维护了一份云服务状态的模板，模板编排成什么样子的，云服务就是什么样子的。对比方式三的优势是我们只需要专注于编排结果即可，不需要关心用什么命令去操作。 Terraform的意义在于，通过同一套规则和命令来操作不同的云平台（包括私有云）。 Terraform知识准备：核心文件有2个，一个是编排文件，一个是状态文件 main.tf文件：是业务编排的主文件，定制了一系列的编排规则，后面会有详细介绍。 terraform.tfstate：本地状态文件，相当于本地的云服务状态的备份，会影响terraform的执行计划。 如果本地状态与云服务状态不一样时会怎样？ 这个大家不需要担心，前面介绍过Terraform是目的式的编排，会按照预设结果完成编排并最终同步更新本地文件。 Provider：Terraform定制的一套接口，跟OpenStack里Dirver、Java里Interface的概念是一样的，阿里云、AWS、私有云等如果想接入进来被Terraform编排和管理就要实现一套Provider，而这些实现对于Terraform的顶层使用者来说是无感知的。 Module：可以理解为provider的集合，完成一个完整的功能。 相关命令：初始化初始化本地环境，下载provider,校验terraform版本等. 12$ terraform init //自动下载最新的provider$ terraform init -plugin-dir //指定provider目录 plan比较云端资源和本地state资源. 1$ terraform plan 部署将修改部署到云端资源. 1$ terraform apply 删除将云端资源删除. 1$ terraform destory Example创建阿里云用户组、资源组并且配置只读权限。注: alicloud : 阿里云provider名字，不能修改。 1234567891011121314151617181920212223242526provider &quot;alicloud&quot; &#123; access_key = &quot;*******************&quot; secret_key = &quot;*************************&quot; region = &quot;cn-beijing&quot; &#125; resource &quot;alicloud_ram_group&quot; &quot;group&quot; &#123; name = &quot;test_group_1000&quot; force = true &#125; resource &quot;alicloud_ram_group_policy_attachment&quot; &quot;attach&quot; &#123; policy_name = &quot;ReadOnlyAccess&quot; policy_type = &quot;System&quot; group_name = alicloud_ram_group.group.name &#125; resource &quot;alicloud_resource_manager_resource_group&quot; &quot;example&quot; &#123; resource_group_name = &quot;tftest01&quot; display_name = &quot;tftest01&quot; &#125; data &quot;alicloud_account&quot; &quot;example&quot; &#123;&#125; resource &quot;alicloud_resource_manager_policy_attachment&quot; &quot;example&quot; &#123; policy_name = &quot;ReadOnlyAccess&quot; policy_type = &quot;System&quot; principal_name = format(&quot;%s@group.%s.onaliyun.com&quot;, alicloud_ram_group.group.name, data.alicloud_account.example.id) principal_type = &quot;IMSGroup&quot; resource_group_id = alicloud_resource_manager_resource_group.example.id &#125; 其他1、配置terraform自动补全 1$ terraform -install-autocomplete 2、查看terraform的日志 12$ export TF_LOG=TRACE$ export TF_LOG_PATH=/var/log/terraform.log 3、terraform 通过配置文件或者环境变量进行配置文件目录(自己创建) 1$ $HOME/.terraformrc 环境变量 1$ os.Setenv(&quot;TF_PLUGIN_CACHE_DIR&quot;, &quot;/tmp/provider&quot;) // add provider cache state文件存储，支持consul、oss等12345678910111213terraform &#123; backend &quot;consul&quot; &#123; address = &quot;consul.example.com&quot; scheme = &quot;https&quot; path = &quot;full/path&quot; &#125;&#125;data &quot;terraform_remote_state&quot; &quot;foo&quot; &#123; backend = &quot;consul&quot; config = &#123; path = &quot;full/path&quot; &#125;&#125; 参考： https://theithollow.com/2018/05/21/using-hashicorp-consul-to-store-terraform-state/ 语法 单行注释以 # 开头 多行注释用 /* 和 */ 换行 值使用 key = value 的语法分配（空格无关紧要）。该值可以是任何原语（字符串，数字，布尔值），列表或映射。 字符串为双引号。 字符串可以使用 ${} 包装的语法对其他值进行插值，例如 ${var.foo} 。此处记录了完整的内插语法。 多行字符串可以使用外壳样式的“ here doc”语法，该字符串以类似 &lt;&lt;EOF 的标记开头，然后以 EOF 结尾。字符串和结束标志的线路必须不能缩进。 假定数字以10为底。如果为数字加上 0x 前缀，则将其视为十六进制数字。 布尔值： true ， false 。 原始类型的列表可以用方括号（ [] ）制成。示例： [“foo”, “bar”, “baz”] 。 参考 https://runebook.dev/zh-CN/docs/terraform/-index-#Alicloud https://www.terraform.io/","categories":[{"name":"Cloud","slug":"Cloud","permalink":"https://acodetailor.github.io/categories/Cloud/"}],"tags":[{"name":"terraform","slug":"terraform","permalink":"https://acodetailor.github.io/tags/terraform/"}]}],"categories":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/categories/k8s/"},{"name":"运维","slug":"运维","permalink":"https://acodetailor.github.io/categories/%E8%BF%90%E7%BB%B4/"},{"name":"terraform","slug":"terraform","permalink":"https://acodetailor.github.io/categories/terraform/"},{"name":"Cloud","slug":"Cloud","permalink":"https://acodetailor.github.io/categories/Cloud/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://acodetailor.github.io/tags/k8s/"},{"name":"knative","slug":"knative","permalink":"https://acodetailor.github.io/tags/knative/"},{"name":"oom","slug":"oom","permalink":"https://acodetailor.github.io/tags/oom/"},{"name":"terraform","slug":"terraform","permalink":"https://acodetailor.github.io/tags/terraform/"}]}